{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "from pathlib import Path\n",
    "import sys, os\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "from collections import defaultdict\n",
    "from itertools import product\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "from catboost import Pool, CatBoostRegressor\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '..'))\n",
    "from utils4t import MultipleTimeSeriesCV, format_time\n",
    "sns.set_style('whitegrid')\n",
    "YEAR = 252\n",
    "idx = pd.IndexSlice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "'3.3.3'"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb.__version__"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "MultiIndex: 1689358 entries, ('A', Timestamp('2010-01-04 00:00:00')) to ('ZION', Timestamp('2016-12-30 00:00:00'))\n",
      "Data columns (total 34 columns):\n",
      " #   Column           Non-Null Count    Dtype  \n",
      "---  ------           --------------    -----  \n",
      " 0   dollar_vol       1689358 non-null  float64\n",
      " 1   dollar_vol_rank  1689358 non-null  float64\n",
      " 2   rsi              1675904 non-null  float64\n",
      " 3   bb_high          1671099 non-null  float64\n",
      " 4   bb_low           1671097 non-null  float64\n",
      " 5   NATR             1675904 non-null  float64\n",
      " 6   ATR              1675904 non-null  float64\n",
      " 7   PPO              1665333 non-null  float64\n",
      " 8   MACD             1657645 non-null  float64\n",
      " 9   sector           1689358 non-null  int32  \n",
      " 10  r01              1689357 non-null  float64\n",
      " 11  r05              1689353 non-null  float64\n",
      " 12  r10              1689348 non-null  float64\n",
      " 13  r21              1689337 non-null  float64\n",
      " 14  r42              1689316 non-null  float64\n",
      " 15  r63              1689295 non-null  float64\n",
      " 16  r01dec           1689357 non-null  float64\n",
      " 17  r05dec           1689353 non-null  float64\n",
      " 18  r10dec           1689348 non-null  float64\n",
      " 19  r21dec           1689337 non-null  float64\n",
      " 20  r42dec           1689316 non-null  float64\n",
      " 21  r63dec           1689295 non-null  float64\n",
      " 22  r01q_sector      1689357 non-null  float64\n",
      " 23  r05q_sector      1689353 non-null  float64\n",
      " 24  r10q_sector      1689348 non-null  float64\n",
      " 25  r21q_sector      1689337 non-null  float64\n",
      " 26  r42q_sector      1689316 non-null  float64\n",
      " 27  r63q_sector      1689295 non-null  float64\n",
      " 28  r01_fwd          1689358 non-null  float64\n",
      " 29  r05_fwd          1689358 non-null  float64\n",
      " 30  r21_fwd          1689343 non-null  float64\n",
      " 31  year             1689358 non-null  int64  \n",
      " 32  month            1689358 non-null  int64  \n",
      " 33  weekday          1689358 non-null  int64  \n",
      "dtypes: float64(30), int32(1), int64(3)\n",
      "memory usage: 439.0+ MB\n"
     ]
    }
   ],
   "source": [
    "data = (pd.read_hdf('data/data.h5', 'model_data').sort_index().loc[idx[:, :'2016'], :])\n",
    "data.info(null_counts=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "labels = sorted(data.filter(like='_fwd').columns)\n",
    "features = data.columns.difference(labels).tolist() # extracting labels that contain _fwd\n",
    "\n",
    "tickers = data.index.get_level_values('symbol').unique()\n",
    "\n",
    "# predict 1, 5, 21 day returns\n",
    "lookaheads = [1, 5, 21]\n",
    "categoricals = ['year', 'month', 'sector', 'weekday']\n",
    "\n",
    "# set train period for 4.5y & test period 3, 1 months\n",
    "train_lengths = [int(4.5 * 252), 252]\n",
    "test_lengths = [63, 21]\n",
    "test_params = list(product(lookaheads, train_lengths, test_lengths))\n",
    "\n",
    "results_path = Path('results', 'us_stocks')\n",
    "if not results_path.exists():\n",
    "    results_path.mkdir(parents=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "[1134, 252]"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_lengths"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [07:07<00:00, 35.64s/it]\n"
     ]
    }
   ],
   "source": [
    "# Baseline: Linear Regression\n",
    "lr = LinearRegression()\n",
    "\n",
    "lr_metrics = []\n",
    "\n",
    "# iterate over our three CV configuration parameters\n",
    "for lookahead, train_length, test_length in tqdm(test_params):\n",
    "    label = f'r{lookahead:02}_fwd'\n",
    "    df = pd.get_dummies(data.loc[:, features + [label]].dropna(), columns=categoricals, drop_first=True)\n",
    "    X, y = df.drop(label, axis=1), df[label]\n",
    "\n",
    "    n_splits = int(2 * YEAR / test_length)\n",
    "    cv = MultipleTimeSeriesCV(n_splits=n_splits, test_period_length=test_length, lookahead=lookahead,\n",
    "                              train_period_length=train_length)\n",
    "\n",
    "    ic, preds = [], []\n",
    "    for i, (train_idx, test_idx) in enumerate(cv.split(X=X)):\n",
    "        X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n",
    "        X_test, y_test = X.iloc[test_idx], y.iloc[test_idx]\n",
    "        lr.fit(X_train, y_train)\n",
    "        y_pred = lr.predict(X_test)\n",
    "        preds.append(y_test.to_frame('y_true').assign(y_pred=y_pred))\n",
    "        ic.append(spearmanr(y_test, y_pred)[0])\n",
    "    preds = pd.concat(preds)\n",
    "    lr_metrics.append([lookahead, train_length, test_length, np.mean(ic),\n",
    "                       spearmanr(preds.y_true, preds.y_pred)[0]])\n",
    "\n",
    "columns = ['lookahead', 'train_length', 'test_length', 'ic_by_day', 'ic']\n",
    "lr_metrics = pd.DataFrame(lr_metrics, columns=columns)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "Text(0, 0.5, '')"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 1008x360 with 2 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAAFJCAYAAAChJA1XAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAAsTAAALEwEAmpwYAAAvgklEQVR4nO3deXBUZaL+8aezkqQJsUFZhGBMCIMiQsCqURYRYQwpEPASTOAiM+C4DUQBIxiRQYhsCqNRwQKVQS5CYNA7XEBUdmEYkQwZZJXAJVIugGkZ0glk6/79waHv8MO2s3WfLN9PFSXp033eJx3D20+/55y2uFwulwAAAAAACjA7AAAAAADUFRQkAAAAADBQkAAAAADAQEECAAAAAAMFCQAAAAAMFCQAAAAAMASZHQCob/r166fXX39dd9xxhyRp+/bteu+991RYWKiysjJ16NBBU6ZMUevWra97bMeOHbV3717ZbLZqjx0cHKwmTZrI5XKpoqJC/fr109NPP62gIH6dAaCxq8kcJUl79+7VokWLdPbsWTVp0kTNmzfXH/7wB/Xo0cPn2b/44gvNmjVLGzZs0NSpU9WhQweNGzfO5+MC/z9eUQE18D//8z9avHixFi9erPbt28vlcmnJkiV65JFHtHHjRoWEhNT6mK+++qp74isuLtazzz6rOXPm6MUXX6z1sQAA9VdV56itW7dq7ty5mj9/vrp16yZJys3N1cSJEzVjxgzde++9ZnwbgN9xiB1QA3/605/0wgsvqH379pIki8Wixx57TGlpaSotLf3Zx7z22msaNmyYhgwZou3bt0uSfve73yk7O9t9n8WLF2v27Nlexw8PD9f06dOVnZ0th8Oh4uJiPffccxoxYoQeeOABPfTQQzp16pS+++47devWTYWFhZIkl8ulBx54QMeOHavpUwAAqKOqOkfNnz9f06ZNc5cjSeratasyMjI0f/58FRYWKiEhQefPn3dvHzFihHbu3KnS0lLNnj1bw4YN04MPPqipU6fK4XBIurKq9cwzz2jgwIH67LPPtH37dqWkpOihhx5S37599dprr/n2iQCqiIIEVNNPP/2kb7/9VgkJCdfcbrFYNHjwYFmt1p99XNu2bfXRRx/plVde0dSpU2W32zVq1CitXbtWkuR0OrV27VqlpKRUKkerVq1ktVp16tQp7dq1S5GRkVqzZo0++eQTde7cWStXrlSbNm109913a/369ZKkv//974qKitKvfvWrGjwDAIC6qqpz1E8//aTTp0/rrrvuum5fd999t/Ly8uR0OjVgwAD3XHLy5EmdP39evXv31pIlSxQYGKgPP/xQ69ev10033aRXX33VvY8OHTro448/Vv/+/fXee+9p7ty5+vDDD5Wdna0lS5bIbrf74FkAqodD7IBqCgi48v6C0+ms0uNSU1MlSfHx8YqNjdWBAwd03333KTMzU8eOHdPZs2fVtm1b3XrrrZXep8ViUVhYmBITE9WuXTutWLFC+fn52rdvn/udwFGjRumVV17RqFGjlJ2d7c4BAGh4qjtHlZeXX3fb1dUmi8Wi5ORkvfTSSxo3bpzWrVunhx56SAEBAdqxY4cKCwv1t7/9TZJUVlam5s2bu/dx9Rwmi8Wit99+Wzt27NCGDRt08uRJuVwuXbp0qVrfJ+ALrCAB1dSsWTPdcsst+uc//3ndtqefftrj4WtXJy3pyqFuQUFBCgwMVEpKiv7yl79o3bp1lV49kqRvv/1WxcXFio6O1gcffKAXXnhBTZo00eDBgzVo0CC5XC5J0j333KNLly5p79692r9/vwYOHFjF7xgAUF9UdY664YYbFBMTo3379l13/y+++EKxsbGKjIxUjx49VF5eroMHD2rDhg36j//4D0lXilhGRob++te/6q9//avWrl2r119/3b2P8PBwSVfOnR02bJgOHz6s2267Tc8995yCgoLccxVQF1CQgBoYP368Xn75ZeXn50uSKioqtGjRIh07dszjCtBHH30kSTp8+LDy8/N15513SpKSk5O1ZcsWHT58WAMGDKjU+BcvXtSsWbM0atQohYaGavfu3Ro2bJiSk5MVExOjbdu2qaKiQtKVd+1GjhypF154QYMGDVJoaGhNv30AQB1W1Tnq+eef1+zZs5Wbm+u+7cCBA5o7d66effZZ923JycmaNWuWOnbsqDZt2kiSevXqpZUrV6q0tFROp1MvvviiFi5ceN0Y+fn5cjgceuaZZ9SvXz/t27fP/RigruAQO6AGBg8eLJfLpUmTJqm8vFwlJSW6/fbbtXz5co9XsDtz5oyGDh0qi8WihQsXKioqSpLUvHlzde7cWbGxsQoODvY45rPPPqsmTZooMDBQFRUV+s1vfqMnn3xSkjR27FhNnz5dH374oQIDA3X77bfr66+/dj922LBhmjdvnh5++OHaexIAAHVSVeeoe++9V/PmzdPrr7+uH374QS6XS61atdK8efP061//2n2/oUOHauHChdcUoKeeekrz5s3TsGHDVFFRoU6dOmnq1KnXjdGxY0f17dtXAwcOVGRkpKKjoxUXF6f8/HyfXPkVqA6LizVNoE6w2+0aPny4Vq5c6fHzKWpq48aN+uijj/TOO+/4ZP8AAAD1HStIQB2wZs0aLVy4UE888YTPytHo0aP1448/6o033vDJ/gEAABoCVpAAAAAAwMBFGgAAAADA4LdD7JxOp2bMmKHjx48rJCREmZmZ7k92lqQ///nP2rhxo6QrJwmOHz9ely9fVnp6ugoKChQREaF58+bJZrP5KzIAAACARsZvK0hbtmxRaWmpsrOzNXnyZM2dO9e97cyZM1q/fr1Wr16tNWvWaPfu3Tp27JhWrVql+Ph4ffDBBxo6dKgWLVrkr7gAAAAAGiG/rSDl5OSod+/ekqSuXbvq0KFD7m2tWrXSO++8o8DAQElXPsU5NDRUOTk5evTRRyVJffr0qVRBys3N5fNdAMBkJSUl6tq1q9kx6iTmKQCoGzzNVX5bQXI4HLJare6vAwMDVV5eLkkKDg6WzWaTy+XSvHnzdNtttykmJkYOh0NNmzaVJEVERKiwsNBfcQEANUABAADUdZ7mKr+tIFmtVhUVFbm/djqdCgr6v+FLSkqUkZGhiIgI/fGPf7zuMUVFRYqMjPQ6TmhoqDp16lTL6QEAVXH06FGzI9RZzFMAUDd4mqv8toKUkJCgXbt2SbpyeEF8fLx7m8vl0lNPPaWOHTtq5syZ7kPtEhIStHPnTknSrl271L17d3/FBQAAANAI+W0FacCAAdqzZ49SUlLkcrk0e/ZsLVu2TNHR0XI6ndq3b59KS0v1+eefS5ImTZqk1NRUTZkyRampqQoODtaCBQv8FRcAAABAI+S3ghQQEKCZM2dec1tsbKz771999dXPPi4rK8unuQAAAADgKj4oFgAAAAAMFCQAAAAAMFCQAAAAAMBAQQIAAAAAAwUJAAAAAAx+u4od0FBs3rxZmzZt8tn+7Xa7JMlms/lsjKSkJCUmJvps/wAAAPUVBQmoYwoKCiT5tiABAADg51GQgCpKTEz06epLWlqaJD4DDAAAwAycgwQAAAAABgoSAAAAABgoSAAAAABgoCABAAAAgIGCBAAAAAAGChIAAAAAGChIAAAAAGCgIAEAAACAgYIEAAAAAAYKEgAAAAAYKEgAAAAAYKAgAQAAAICBggQAAAAABgoSAAAAABgoSAAAAABgoCABAAAAgCHI7AAAAAAAvNu8ebM2bdrkk33b7XZJks1m88n+JSkpKUmJiYk+239toSABAAAAjVxBQYEk3xak+oKCBAAAANQDiYmJPluBSUtLkyRlZWX5ZP/1CecgAQAAAICBggQAAAAABg6xQ4OUlZWlvLw8s2NUy4kTJyT931J3fRMXF1dvswMAAFCQ0CDl5eXp60P/ULS1wuwoVRbpskiSLp/+0uQkVfeNI9DsCAAAADVCQUKDFW2t0LQeDrNjNCqZ+61mRwAAAKgRzkECAAAAAAMFCQAAAAAMFCQAAAAAMFCQAAAAAMBAQQIAAAAAAwUJAAAAAAx+u8y30+nUjBkzdPz4cYWEhCgzM1Pt27e/5j52u12pqalav369QkND5XK51KdPH91yyy2SpK5du2ry5Mn+igwAAACgkfFbQdqyZYtKS0uVnZ2t3NxczZ07V4sXL3Zv//zzz7VgwQKdP3/efds333yj22+/XW+//ba/YgIAAABoxPx2iF1OTo569+4t6cpK0KFDh64NEhCgZcuWKSoqyn3b4cOHdfbsWY0ePVq///3vderUKX/FBQAAANAI+W0FyeFwyGq1ur8ODAxUeXm5goKuROjZs+d1j7nxxhv12GOPaeDAgdq/f7/S09O1bt26XxynpKRER48erd3wqHeKi4s5wc4kxcXF/A4Cv4B5CkBdVFxcLEn8+yQ/FiSr1aqioiL3106n012OPOncubMCAwMlST169NC5c+fkcrlksVg8PiY0NFSdOnWqndCot8LDw3XZ7BCNVHh4OL+DYIL9BcxTAOqi8PBwSWpU/z55mqv89iZ7QkKCdu3aJUnKzc1VfHy818e8+eabWr58uSTp2LFjat269S+WIwAAAACoCb+tIA0YMEB79uxRSkqKXC6XZs+erWXLlik6Olr333//zz7mscceU3p6unbu3KnAwEDNmTPHX3EBAAAANEJ+K0gBAQGaOXPmNbfFxsZed79t27a5/96sWTMtWbLE59kAQJI2b96sTZs2+WTfdrtdkmSz2Xyyf0lKSkpSYmKiz/YPAEBj4LeCBACNWUFBgSTfFiQAAFBzFCQAMCQmJvpsBSYtLU2SlJWV5ZP9AwCA2sGVkAEAAADAQEECAAAAAAMFCQAAAAAMFCQAAAAAMFCQAAAAAMBAQQIAAAAAAwUJAAAAAAwUJAAAAAAwUJAAAAAAwEBBAgAAAABDkNkBAKCysrKylJeXZ3aMajlx4oQkKS0tzeQk1RcXF1ev8wMAUBkUJAD1Rl5eng4cPiBFmZ2kGoz1+gPfHjA3R3VdMDsAAAD+QUECUL9ESc6+TrNTNDoBOzgiGwDQODDjAQAAAICBggQAAAAABgoSAAAAABgoSAAAAABgoCABAAAAgIGCBAAAAAAGChIAAAAAGChIAAAAAGCgIAEAAACAIcjsAABQWXa7XbogBezgvR2/uyDZw+xmpwAAwOd4lQEAAAAABlaQ0CDZ7XadLwxU5n6r2VEalfzCQN1o990qg81mU/6lfDn7On02Bn5ewI4A2Ww2s2MAAOBzrCABAAAAgIEVJDRINptN4RdPaloPh9lRGpXM/VY1YZUBAADUYxQkAAAAoBZkZWUpLy/P7BjVcuLECUlSWlqayUmqLy4urlbyU5AAAACAWpCXl6cDhw9IUWYnqQbjxJsD3x4wN0d1Xai9XVGQAAAAgNoSJS4mZILa/AgQLtIAAAAAAAYKEgAAAAAYvBak7du3X/P1pk2bfBYGAAAAAMzk8Ryk7du36x//+Ic2btyoAweunKxVUVGhbdu2KSkpyW8BAQAAAMBfPBakX/3qV7pw4YJCQ0MVExMjSbJYLBo0aJDfwgEAAACAP3ksSK1bt9awYcM0ZMgQBQRwqhIAAACAhs/rZb6XLl2qpUuXqkmTJu7bdu/eXeWBnE6nZsyYoePHjyskJESZmZlq3779Nfex2+1KTU3V+vXrFRoaqsuXLys9PV0FBQWKiIjQvHnzZLPZqjw2AAAAAFSG16WhjRs36vPPP9fu3bvdf6pjy5YtKi0tVXZ2tiZPnqy5c+des/3zzz/X2LFjdf78efdtq1atUnx8vD744AMNHTpUixYtqtbYAAAAAFAZXgtS27Ztr1k9qq6cnBz17t1bktS1a1cdOnTo2iABAVq2bJmioqJ+9jF9+vTR3r17a5wDAAAAADzxeohdWVmZBg8erPj4eFksFknSggULqjyQw+GQ1Wp1fx0YGKjy8nIFBV2J0LNnz599TNOmTSVJERERKiws9DpOSUmJjh49WuV8aFiKi4v5kC+TFBcX++x3sLi4WLpQu5+W7TeXjf/W/P0mc1yQim/w3c+2MWGeAhqu4uJisyM0arX1GsRrQfr9739f40EkyWq1qqioyP210+l0l6PKPKaoqEiRkZFexwkNDVWnTp1qFhb1Xnh4uPv1KPwrPDzcZ7+DXbp0UXh4uE/27WsnTpyQJHW4uYPJSarpZikuLq7SP1sKgGfMU0DDFR4eLv1kdorGq6qvQTzNVV4L0m233aalS5fq3Llzuu+++9SxY8fKp/w3CQkJ2r59u5KSkpSbm6v4+PhKPWbnzp3q0qWLdu3ape7du1drbAANQ1pamtkRqu1q9qysLJOTAACAX+L1OJWMjAy1a9dO+fn5atGihV544YVqDTRgwACFhIQoJSVFc+bM0fPPP69ly5Zp69atHh+TmpqqEydOKDU1VdnZ2Ro/fny1xgYAAACAyvC6gnThwgUNHz5c69evV0JCgpxOZ7UGCggI0MyZM6+5LTY29rr7bdu2zf33sLAw3m0FAAAA4DeVOtP55MmTkqQffvhBgYGBPg0EAAAAAGbxWpBeeOEFZWRk6MiRI0pLS9PUqVP9kQsAAAAA/M7rIXYdO3ZUdna2P7IAAAAAgKk8FqS0tDRlZWWpV69e123bvXu3T0MBAAAAgBk8FqSrF0fYvXu3iouLFR4errNnz6ply5Z+CwcAAAAA/uT1HKQ333xTb7/9tiTp5Zdf1pIlS3weCgAAAADM4LUgbdu2TZMmTZJ0ZVXp3y/DDQAAAAANideCZLFYVFpaKkkqKyuTy+XyeSgAAAAAMIPXq9ilpKRo8ODBio+P16lTp/Too4/6IxcAAAAA+J3XgpScnKz7779fZ86cUbt27WSz2fyRCwAAAAD8zmNBWrRokZ566ilNmjRJFovlmm0LFizweTAAAAAA8DePBclqtUqShg4dqiZNmvgtEAAAAACYxWNBWrdunYYPH66lS5fqvffe4+IMAAAAABo8jwWpd+/eevDBB3Xu3DklJiZKklwulywWi7Zu3eq3gAAAAADgLx4LUqtWrbRlyxa9+eabGj9+vD8zAQAAAIApPBakFStWqG3btvrss8/UrVu3aw6x69Wrl1/CAQAAAIA/eSxI6enp+vTTT1VQUKANGzZcs42CBAAAAKAh8liQ+vfvr/79+2vbtm3q16+fLly4oGbNml13yW8AAAAAaCi8flCs1WrVoEGDVFFRocTERLVp00bJycn+yAbUyDeOQGXut5odo8r+VXrlTYhmIfXvypHfOAIVb3aIGti8ebM2bdrkk32fOHFCkpSWluaT/UtSUlKS+6I6AACgerwWpNdff13/9V//pQkTJuiJJ55QamoqBQl1XlxcnNkRqu2M8UK65S0dTE5SdfGq38+9LzVv3tzsCAAAoBK8FqSAgABFRUXJYrEoNDRUERER/sjVIPjy3WhJstvtkiSbzeaT/dfnd6N9+S69r13NnpWVZXKSxicxMbHe/j8PAABqh9eCFB0drQULFuinn37SkiVL1KZNG3/kQiUUFBRI8l1BAgAAABobrwXppZde0tq1a9WjRw+Fh4dr1qxZ/sjVIPj63WhWGgAAAIDaFeDtDhaLRU6nUy6XSxUVFf7IBAAAAACm8LqC9OKLLyoyMlK9evXSvn37NG3aNM2fP98f2YA6ydfnlnG1MwAAAPN4LUj5+flauXKlpCufjZSSkuLzUEBjxtXOAAAAzOO1IJWUlOjSpUsKCwvT5cuXOcwOjR5XOgMAAGi4vBakRx55REOGDFGHDh2Ul5dXry+fDAAAAAC/xGtBevDBB9WnTx+dOXNGbdu21Q033OCPXAAAAADgdx6vYudwODR58mQ5HA5FRUXp9OnTmjlzphwOhz/zAQAAAIDfeCxIf/zjH3XHHXcoIiJCkjRw4EB17txZM2bM8Fc2AAAAAPArjwXpu+++029/+1tZLBZJUlBQkMaNG6czZ874LRwAAAAA+JPHc5CCgn5+U3BwsM/C+FtWVpby8vLMjlFt/vi8HF+Ki4urt9kBAADQMHksSNHR0dqyZYv69+/vvm3r1q268cYb/RLMH/Ly8nTgqyNyhtvMjlItloorP76ckz+YnKTqAortZkcAAAAAruOxIE2ZMkWTJk3SW2+9pbZt2+r777+XzWbT/Pnz/ZnP55zhNl2+bZDZMRqdJkc2mB0BAAAAuI7HghQZGal33nlH3333nc6dO6fWrVurZcuW/swGAAAAAH7l9XOQ2rRpozZt2vgjCwAAAGpo8+bN2rRpk8/2b7dfOUzeZvPdKQpJSUlKTEz02f6BX+K1IAEAAABXFRQUSPJtQQLM5LeC5HQ6NWPGDB0/flwhISHKzMxU+/bt3dvXrFmj1atXKygoSE8++aTuu+8+XbhwQQ888IDi4+MlSf3799eYMWP8FRkAAKDeSUxM9Onqy9Ur0GZlZflsDMBMXgvSnj17tGzZMpWWlrpve//996s80JYtW1RaWqrs7Gzl5uZq7ty5Wrx4sSTp/PnzWrFihdatW6eSkhKNHDlSPXv21JEjRzRo0CC9+OKLVR4PAAAAAKrKa0GaM2eOMjIy1KpVqxoNlJOTo969e0uSunbtqkOHDrm3HTx4UN26dVNISIhCQkIUHR2tY8eO6dChQzp8+LD+8z//UzabTdOmTdNNN91UoxwAAAAA4InXgtS6dWvdc889NR7I4XDIarW6vw4MDFR5ebmCgoLkcDjUtGlT97aIiAg5HA7deuut6ty5s+655x6tX79emZmZXpdzS0pKdPTo0UplKi4urt43g1pRXFxc6Z8VADQUVZmngLro6usn/j++Hq8tzVVbry29FqTmzZtr+vTpuu2222SxWCRJDz/8cJUHslqtKioqcn/tdDoVFBT0s9uKiorUtGlTdenSRWFhYZKkAQMGVOpY19DQUHXq1KlSmcLDwyVdrMJ3gdoUHh5e6Z8VgPqFF06eVWWeAuqiK6+fxP/HPyM8PFz6yewUjVdVX1t6mqsCvD2wbdu2uummm/Tjjz/q/PnzOn/+fOVT/puEhATt2rVLkpSbm+u+8IIkdenSRTk5OSopKVFhYaFOnjyp+Ph4TZs2TZ988okkae/evbr99turNTYAAAAAVIbXFaTx48drx44dOnHihGJiYtS/f/9qDTRgwADt2bNHKSkpcrlcmj17tpYtW6bo6Gjdf//9Gj16tEaOHCmXy6WJEycqNDRUkydPVkZGhlatWqWwsDBlZmZWa2wAAAAAqAyvBWnBggXKz89XQkKC/vu//1s5OTmaMmVKlQcKCAjQzJkzr7ktNjbW/fcRI0ZoxIgR12xv166dVqxYUeWxKstutyuguEBNjmzw2Rj4eQHFBbLbQ8yOAQAAAFzDa0H68ssvtXr1aknSmDFjrisxAAAAqJqsrCzl5eWZHaNaTpw4Ien/Pg+pvomLi6u32eEfXgtSeXm5nE6nAgIC5HK53BdqaAhsNpv+96dSXb5tkNlRGp0mRzbwCdwAgEYrLy9PXx/6h6KtFWZHqbJI15XXgpdPf2lykqr7xhFodgTUA14LUlJSklJTU3XnnXfq4MGDSkpK8kcuAACABi3aWqFpPRxmx2hUMvdbvd8JjZ7XgjR27Fj16tVLp06d0vDhw6+5+hwAAACAK+x2u3RBCtjh9ULRqG0XJHuYvVZ25bEgrV27VsnJyVqwYIH7sLojR45IkiZNmlQrgwMAAABAXeKxILVq1UqSdOutt15ze0M6BwkAAACoLTabTfmX8uXs6zQ7SqMTsCOg1s5v97j+17t3b0nSV199pWHDhrn//O1vf6uVgQEAAACgrvG4grRy5UotXrxY//rXv/Tpp5+6b//3zy4CAAAAgIbEY0EaNWqURo0apbfffltPPPGEPzMBAAAAgCm8XsUuJSVFGzZsUHl5uVwul86dO6fHH3/cH9n8IqDYriZHNpgdo1osZZckSa7gMJOTVF1AsV1SK7NjAAAAANfwWpDGjx+vW2+9VV9//bVCQ0MVFlb/Xox7EhcXZ3aEGrn6SdYdYutj0WhV759/AAAANDxeC5LL5dLMmTP1/PPP6+WXX9bIkSP9kcsv0tLSzI5QI1fzZ2VlmZwEAAAAaBi8fopVYGCgSkpKdOnSJVksFlVUVPgjFwAAAAD4ndeCNGrUKP35z39Wz549de+996pt27b+yAUAAAAAfuf1ELsHHnjA/feBAwfKarX6NBAAAAAAmMVrQVq9erVWr16t0tJS922bNm3yaaiGYvPmzT59rq5epMFX51IlJSUpMTHRJ/sGAAAA6iKvBen999/XkiVL1KxZM3/kQRU0b97c7AgAAABAg+K1IHXs2FGtW7dWYGCgP/I0KImJiazAAAAarR9//FEvvfSSZsyYwZt6AOoNrwXp17/+tfr376927drJ5XLJYrHo/fff90c2AABQjy1fvlwHDx7U8uXLNWnSJLPjAECleC1I2dnZeu2119S0aVN/5AEAAA3Ajz/+qI8//lgul0sff/yxxowZwyoSgHrB62W+W7ZsqTvuuEO33nqr+w8AAMAvWb58uVwulyTJ6XRq+fLlJicCgMrxuoJUWlqqIUOGqEOHDrJYLJKkBQsW+DwYAACovz777DOVlZVJksrKyvTpp59ymB2AesFrQUpNTVVkZKQ/sgAAgAZiwIAB2rRpk8rKyhQcHKzf/OY3ZkcCgErxWpDeffddrVq1yh9ZAABAAzFmzBh9/PHHkqSAgACNGTPG5EQAUDleC1KzZs20fPlyxcTEKCDgyilLvXr18nkwAABQf7Vo0UIDBw7U+vXrNXDgQC7QAKDe8FqQbrjhBh07dkzHjh1z30ZBAgAA3owZM0anT59m9QhAveK1IM2ZM0dff/218vLyFBMTo06dOvkjFwAAqOdatGihN954w+wYAFAlXgvSihUrtGHDBnXp0kXvvfeeBg4cqHHjxvkjGwAAAAD4ldeCtGHDBq1cuVJBQUEqKytTSkoKBQkAAABAg+S1ILlcLgUFXblbcHCwgoODfR4KAACgIbPb7TpfGKjM/VazozQq+YWButFuNzsG6jivBSkhIUFpaWnq3r27cnJy1K1bN3/kAgAAAAC/81iQvvzyS911112aOHGi9u7dq5MnT+qhhx5S3759/RgPAACg4bHZbAq/eFLTejjMjtKoZO63qonNZnYM1HEBnjZkZmaquLhYjz76qHr27KnRo0frnnvuUWlpqT/zAQAAAIDfeFxB6tWrlx588EGdO3dOiYmJkq6cj2SxWLR161a/BQQAAAAAf/FYkNLT05Wenq633npLf/jDH/yZCQAAAABM4fUiDcOGDdPSpUtVUlLivm38+PE+DQUAAAAAZvB4DtJVzzzzjBwOh1q0aOH+AwAAAAANkdcVpIiICE2cONEfWQAAAADAVF4LUocOHbRx40Z16tRJFotFkhQTE1PlgZxOp2bMmKHjx48rJCREmZmZat++vXv7mjVrtHr1agUFBenJJ5/UfffdJ7vdrmeffVaXL1/WTTfdpDlz5igsLKzKYwMAUB9lZWUpLy/PZ/u32+0qKCjw2f59rXnz5rL58JLNcXFxSktL89n+AdRNXgvS0aNHdfToUffXFotF77//fpUH2rJli0pLS5Wdna3c3FzNnTtXixcvliSdP39eK1as0Lp161RSUqKRI0eqZ8+eWrRokQYNGqSHHnpIS5YsUXZ2tn77299WeWwAAOqjvLw8HfjqiJzhvikBlrJLspTV34/vKDz3L/3vT77JH1Bs98l+AdR9XgvSihUramWgnJwc9e7dW5LUtWtXHTp0yL3t4MGD6tatm0JCQhQSEqLo6GgdO3ZMOTk5evzxxyVJffr00cKFCylIAIBGxRlu0+XbBpkdo9FpcmSD2REAmMRjQXr44Yfdh9T9/1avXl3lgRwOh6xWq/vrwMBAlZeXKygoSA6HQ02bNnVvi4iIkMPhuOb2iIgIFRYWeh2npKTkmhUvAADqkqrMU8XFxT5Og19SXFzss9cUxcXF3q+UBZ/w9c8V5qmtn63HgrRw4cIa7/zfWa1WFRUVub92Op0KCgr62W1FRUVq2rSp+/YmTZqoqKhIkZGRXscJDQ1Vp06dajU7AKBqeKPKs6rMU+Hh4ZIu+jYQPAoPD/fZa4rw8HBd9sme4Y2vf676ySe7RiVU9Wfraa7y+ObFzTff7PFPdSQkJGjXrl2SpNzcXMXHx7u3denSRTk5OSopKVFhYaFOnjyp+Ph4JSQkaOfOnZKkXbt2qXv37tUaGwAAAAAqw+s5SLVlwIAB2rNnj1JSUuRyuTR79mwtW7ZM0dHRuv/++zV69GiNHDlSLpdLEydOVGhoqJ588klNmTJFa9as0Q033KAFCxb4Ky4AAACARshvBSkgIEAzZ8685rbY2Fj330eMGKERI0Zcs71FixZ69913/ZIPAAAAADg/EAAAAAAMfltBAgAAVWO32xVQXMAlp00QUFwguz3E7BgATMAKEgAAAAAYWEECAKCOstls+t+fSvmgWBM0ObJBNpvN7BgATEBBAgCgDgsottfbQ+wsZZckSa7gMJOTVF1AsV1SK7NjADABBQkAgDoqLi7O7Ag1cuLECUlSh9j6WDRa1fvnH0D1UJAAAKij0tLSzI5QI1fzZ2VlmZwEACqPizQAAAAAgIGCBAAAAAAGDrEDAKCR2rx5szZt2uSz/V89B8lXhwomJSUpMTHRJ/sG0HhRkAAAgE80b97c7AgAUGUUJAAAGqnExERWYEz0jSNQmfutZseosn+VWiRJzUJcJiepum8cgYo3OwTqPAoSAACAn9XnS4ifMQ6dbHlLB5OTVF286vdzD/+gIAEAAPhZfb6EO5dvR0PHVewAAAAAwEBBAgAAAAADBQkAAAAADJyDBAAA0IDU98+3kviMK5iLggQAAIBK4/Ot0NBRkAAAABoQPt8KqBnOQQIAAAAAAwUJAAAAAAwcYgcAAADUlgtSwI56uAZx2fhvE1NTVN8FSTfXzq4oSAAAAEAtiIuLMztCtV29OmGHmzuYnKSabq6955+CBAAAANQCX1763NeuZs/KyjI5ifnq4fofAAAAAPgGBQkAAAAADBQkAAAAADBQkAAAAADAQEECAAAAAAMFCQAAAAAMFCQAAAAAMFCQAAAAAMBAQQIAAAAAAwUJAAAAAAwUJAAAAAAwUJAAAAAAwEBBAgAAAABDkL8Gunz5stLT01VQUKCIiAjNmzdPNpvtmvu8+eab2rFjh4KCgpSRkaEuXbroyJEjevzxx3XLLbdIklJTU5WUlOSv2AAAAAAaEb8VpFWrVik+Pl4TJkzQxo0btWjRIk2bNs29/fDhw9q3b5/Wrl2r77//XhMmTNC6det0+PBh/e53v9PYsWP9FRUAAABAI+W3Q+xycnLUu3dvSVKfPn20d+/e67b36tVLFotFbdq0UUVFhex2uw4dOqQdO3Zo1KhRysjIkMPh8FdkAAAAAI2MT1aQ1q5dq+XLl19zW/PmzdW0aVNJUkREhAoLC6/Z7nA4FBUV5f766n26dOmi5ORkde7cWYsXL9Zbb72lKVOmeBy7pKRER48erb1vBgCAWsQ8BaAuKi4uliT+fZKPClJycrKSk5OvuW38+PEqKiqSJBUVFSkyMvKa7Var1b396n2aNm2qAQMGuO87YMAAzZo16xfHDg0NVadOnWrj2wAAVBMTrGfMUwDqovDwcElqVP8+eZqr/HaIXUJCgnbu3ClJ2rVrl7p3737d9t27d8vpdOq7776T0+mUzWbTuHHjdPDgQUnS3r17dfvtt/srMgAAAIBGxm8XaUhNTdWUKVOUmpqq4OBgLViwQJI0f/58JSYmqkuXLurRo4cefvhhOZ1OTZ8+XZI0Y8YMzZo1S8HBwWrRooXXFSQAAAAAqC6Ly+VymR2iNh09erRRLQ0CQF3Ev8We8dwAqIvS0tIkSVlZWSYn8R9P/x7zQbEAAAAAYKAgAQAAAICBggQAAAAABgoSAAAAABgoSAAAAABgoCABAAAAgIGCBAAAAAAGChIAAAAAGChIAAAAAGCgIAEAAACAgYIEAAAAAAYKEgAAAAAYKEgAAAAAYKAgAQAAAICBggQAAAAABgoSAAAAABgoSAAAAABgoCABAAAAgIGCBAAAAAAGChIAAAAAGChIAAAAAGCgIAEAAACAIcjsAAAAAAC827x5szZt2uSTfZ84cUKSlJaW5pP9S1JSUpISExN9tv/aQkECAAAAGrnmzZubHaHOoCABAAAA9UBiYmK9WIGp7zgHCQAAAAAMFCQAAAAAMFCQAAAAAMBAQQIAAAAAAwUJAAAAAAwUJAAAAAAwUJAAAAAAwEBBAgAAAAADBQkAAAAADBQkAAAAADBQkAAAAADAEGR2gNpWUlKio0ePmh0DABq1kpISsyPUWcxTAFA3eJqrLC6Xy+XnLAAAAABQJ3GIHQAAAAAYKEgAAAAAYKAgAQAAAICBggQAAAAABgoSAAAAABga3GW+G5N//vOfevXVV7VixQqzo6AWDRs2TFarVZLUtm1bzZkzx+REqI6ysjJlZGTo22+/VWlpqZ588kndf//9kqTZs2crJiZGqampJqcEfIt5qmFinmoYmKc8oyDVU0uXLtX69esVFhZmdhTUopKSErlcLl5MNADr169XVFSUXnnlFV24cEFDhw5Vt27d9Nxzz+n06dMaN26c2REBn2KeapiYpxoO5inPOMSunoqOjtYbb7xhdgzUsmPHjunSpUsaO3asHnnkEeXm5podCdWUmJiop59+WpLkcrkUGBiooqIiTZgwQUOGDDE5HeB7zFMNE/NUw8E85RkFqZ564IEHFBTEAmBD06RJE40bN07vvvuuXnrpJT377LMqLy83OxaqISIiQlarVQ6HQ2lpaXrmmWfUrl073XnnnWZHA/yCeaphYp5qOJinPKMgAXVITEyMHnzwQVksFsXExCgqKkrnz583Oxaq6fvvv9cjjzyiIUOGaPDgwWbHAYAaY55qWJinfh4FCahD/vKXv2ju3LmSpLNnz8rhcOjGG280ORWq48cff9TYsWOVnp6u4cOHmx0HAGoF81TDwTzlGQUJqEOGDx+uwsJCpaamauLEiZo9ezaHqNRTb7/9ti5evKhFixZp9OjRGj16tC5fvmx2LACoEeaphoN5yjOLy+VymR0CAAAAAOoCVpAAAAAAwEBBAgAAAAADBQkAAAAADBQkAAAAADBQkAAAAADAQEECqunDDz/Uq6++WqXH9OvXTyUlJV7v98UXX2jixInVjXadXbt2aerUqbW2PwBA3cc8BVQPBQkAAAAADHyyF1BD7733njZu3KigoCD16NFD6enpunjxotLT0+VwOFRRUaGnn35ad999t/sxq1at0p49e7Rw4UJt27ZNK1euVHl5uSwWi958801JUn5+vh599FHZ7Xbdd999mjBhgo4fP67MzExJUlRUlGbPnq3w8HBNnz5dP/zwg86dO6d+/fpp4sSJOnnypDIyMhQWFqawsDA1a9bMlOcHAGAu5imgaihIQA3k5+friy++0OrVqxUUFKQJEyZo+/bt2rdvn+655x6NGTNGZ8+eVWpqqrZu3SpJWrFihY4eParXX39dgYGBOn36tJYsWaKwsDBNnz5du3fvVsuWLVVSUqJFixapoqJCffv21YQJE/Tiiy9q9uzZiouL09q1a/XOO+8oOTlZXbt2VXJyskpKStSnTx9NnDhR8+fPV1pamnr27KklS5bo1KlTJj9bAAB/Y54Cqo6CBNTA0aNH1bdvXwUHB0uSevTooRMnTujkyZMaPHiwJKlly5ayWq0qKCiQJO3du1eBgYEKDAyUJDVv3lxTpkxRRESETp06pa5du0qSOnTooJCQEElSUNCVX9WTJ0/qpZdekiSVlZXplltuUVRUlL766iv9/e9/l9VqVWlpqSTp9OnT6tKliyQpISGBiQcAGiHmKaDqOAcJqIFOnTrp4MGDKi8vl8vl0pdffqmYmBjFxsZq//79kqSzZ8/q4sWLioqKkiQtWrRIkZGRWrVqlQoLC5WVlaU//elPyszMVGhoqFwulyTJYrFcN15MTIzmzZunFStWKD09XX379tWHH36opk2basGCBRo7dqwuX74sl8ul2NhYHThwQJJ06NAh/zwhAIA6hXkKqDpWkIAaaN++vRISEpSamiqn06nu3burf//+uuuuu5SRkaFPPvlEly9f1syZM93vrknStGnTlJycrLvvvlsJCQl6+OGHFRQUpMjISJ07d05t27b92fFmzJihKVOmuI8Df/nllxUbG6vJkycrNzdXISEhat++vc6dO6epU6dqypQpevfdd2Wz2RQaGuqvpwUAUEcwTwFVZ3FdfRsAAAAAABo5DrEDAAAAAAMFCQAAAAAMFCQAAAAAMFCQAAAAAMBAQQIAAAAAAwUJAAAAAAwUJAAAAAAwUJAAAAAAwPD/ABqPADb4MLYUAAAAAElFTkSuQmCC\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(ncols=2, figsize=(14,5), sharey=True)\n",
    "\n",
    "# plot average of daily IC values\n",
    "sns.boxplot(x='lookahead', y='ic_by_day',data=lr_metrics, ax=axes[0])\n",
    "axes[0].set_title('IC by Day')\n",
    "\n",
    "# plot IC across all predictions\n",
    "sns.boxplot(x='lookahead', y='ic',data=lr_metrics, ax=axes[1])\n",
    "axes[1].set_title('IC Overall')\n",
    "axes[0].set_ylabel('Information Coefficient')\n",
    "axes[1].set_ylabel('')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# top3 perofrmance in Train/Test Period Lengths\n",
    "(lr_metrics.groupby('lookahead', group_keys=False).apply(lambda x: x.nlargest(3, 'ic_by_day')))\n",
    "lr_metrics.to_csv(results_path / 'lin_reg_metrics.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Parameters: 108\n"
     ]
    }
   ],
   "source": [
    "# LightGBM Model Tuning\n",
    "def get_fi(model):\n",
    "    \"\"\"Return normalized feature importance as pd.Series\"\"\"\n",
    "    fi = model.feature_importance(importance_type='gain')\n",
    "    return (pd.Series(fi / fi.sum(), index=model.feature_name()))\n",
    "\n",
    "# Hyperparameter opt\n",
    "base_params = dict(boosting='gbdt', objective='regression', verbose=-1, num_threads=6, device_type='gpu')\n",
    "\n",
    "# constraints on structure (depth) of each tree\n",
    "max_depths = [2, 3, 5, 7]\n",
    "num_leaves_opts = [2 ** i for i in max_depths]\n",
    "min_data_in_leaf_opts = [250, 500, 1000]\n",
    "\n",
    "# weight of each new tree in the ensemble\n",
    "learning_rate_ops = [.01, .1, .3]\n",
    "\n",
    "# random feature selection\n",
    "feature_fraction_opts = [.3, .6, .95]\n",
    "param_names = ['learning_rate', 'num_leaves', 'feature_fraction', 'min_data_in_leaf']\n",
    "cv_params = list(product(learning_rate_ops, num_leaves_opts, feature_fraction_opts, min_data_in_leaf_opts))\n",
    "n_params = len(cv_params)\n",
    "print(f'# Parameters: {n_params}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train configs: 6\n"
     ]
    }
   ],
   "source": [
    "# Train/Test Period Lengths\n",
    "lookaheads = [1, 5, 21]\n",
    "label_dict = dict(zip(lookaheads, labels))\n",
    "train_lengths = [int(4.5 * 252), 252]\n",
    "test_lengths = [63]\n",
    "test_params = list(product(lookaheads, train_lengths, test_lengths))\n",
    "n = len(test_params)\n",
    "test_param_sample = np.random.choice(list(range(n)), size=int(n), replace=False)\n",
    "test_params = [test_params[i] for i in test_param_sample]\n",
    "print('Train configs:', len(test_params))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# Categorical Variables\n",
    "categoricals = ['year', 'weekday', 'month']\n",
    "for feature in categoricals:\n",
    "    data[feature] = pd.factorize(data[feature], sort=True)[0]\n",
    "\n",
    "# Custom Loss Function: Information Coefficient\n",
    "def ic_lgbm(preds, train_data):\n",
    "    \"\"\"Custom IC eval metric for lightgbm\"\"\"\n",
    "    is_higher_better = True\n",
    "    return 'ic', spearmanr(preds, train_data.get_label())[0], is_higher_better\n",
    "\n",
    "# Run Cross-Validation\n",
    "lgb_store = Path(results_path / 'tuning_lgb.h5')\n",
    "labels = sorted(data.filter(like='fwd').columns)\n",
    "features = data.columns.difference(labels).tolist()\n",
    "label_dict = dict(zip(lookaheads, labels))\n",
    "num_iterations = [10, 25, 50, 75] + list(range(100, 501, 50))\n",
    "num_boost_round = num_iterations[-1]\n",
    "metric_cols = (param_names + ['t', 'daily_ic_mean', 'daily_ic_mean_n',\n",
    "                              'daily_ic_median', 'daily_ic_median_n'] + [str(n) for n in num_iterations])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lookahead:  1 | Train: 1134 | Test: 63 | Params:  54 | Train configs: 6\n",
      "\t  0 | 00:00:55 ( 55) |  0.30 |   4 | 95% | 1000 |   1.89% |  1.55% |  300 |  2.09% |  500\n",
      "\t  1 | 00:01:51 ( 56) |  0.01 |   4 | 60% |  500 |   1.30% |  0.85% |  100 |  0.77% |  200\n",
      "\t  2 | 00:02:46 ( 55) |  0.10 |   4 | 60% |  500 |   3.08% |  2.01% |  450 |  1.79% |  250\n",
      "\t  3 | 00:05:42 (176) |  0.10 | 128 | 95% |  250 |   3.28% |  1.52% |  200 |  1.78% |  150\n",
      "\t  4 | 00:06:48 ( 66) |  0.01 |   8 | 30% |  500 |   1.03% |  1.25% |  350 |  1.01% |  350\n",
      "\t  5 | 00:07:47 ( 59) |  0.10 |   8 | 30% |  250 |   2.17% |  1.41% |  100 |  1.56% |  200\n",
      "\t  6 | 00:10:49 (182) |  0.10 | 128 | 95% |  500 |   3.25% |  1.26% |  500 |  1.42% |   50\n",
      "\t  7 | 00:13:43 (174) |  0.10 | 128 | 60% |  250 |   3.40% |  1.16% |  100 |  1.63% |  250\n",
      "\t  8 | 00:14:44 ( 61) |  0.10 |   8 | 30% | 1000 |   2.95% |  1.54% |  450 |  2.00% |  500\n",
      "\t  9 | 00:15:50 ( 66) |  0.01 |   8 | 30% | 1000 |   1.59% |  1.39% |  400 |  1.01% |  500\n",
      "\t 10 | 00:17:18 ( 89) |  0.30 |  32 | 95% | 1000 |   4.15% |  1.68% |   25 |  1.47% |  500\n",
      "\t 11 | 00:18:11 ( 52) |  0.10 |   4 | 30% |  250 |   3.38% |  1.57% |  350 |  1.67% |  250\n",
      "\t 12 | 00:19:12 ( 62) |  0.30 |   8 | 30% | 1000 |   3.91% |  1.48% |   25 |  1.53% |   25\n",
      "\t 13 | 00:20:12 ( 59) |  0.10 |   8 | 95% |  250 |   3.46% |  1.76% |  500 |  1.89% |  350\n",
      "\t 14 | 00:21:57 (105) |  0.01 |  32 | 30% |  250 |   1.73% |  1.38% |   25 |  1.37% |   25\n",
      "\t 15 | 00:22:50 ( 53) |  0.01 |   4 | 30% |  250 |   0.78% |  0.94% |  500 |  1.31% |  100\n",
      "\t 16 | 00:23:41 ( 51) |  0.30 |   4 | 30% |  250 |   4.12% |  1.78% |  200 |  2.42% |  200\n",
      "\t 17 | 00:25:25 (105) |  0.01 |  32 | 95% | 1000 |   3.01% |  1.60% |  500 |  2.20% |  450\n",
      "\t 18 | 00:26:23 ( 58) |  0.30 |   8 | 30% |  250 |   3.56% |  1.92% |   75 |  2.21% |   75\n",
      "\t 19 | 00:27:24 ( 61) |  0.10 |   8 | 95% | 1000 |   3.32% |  2.19% |  500 |  2.17% |  450\n",
      "\t 20 | 00:30:15 (171) |  0.30 | 128 | 60% |  500 |   3.37% |  1.75% |   50 |  1.89% |   10\n",
      "\t 21 | 00:31:41 ( 86) |  0.30 |  32 | 30% |  500 |   3.27% |  1.51% |  150 |  1.95% |  500\n",
      "\t 22 | 00:32:45 ( 64) |  0.01 |   8 | 60% |  250 |   1.94% |  1.26% |  400 |  1.24% |   75\n",
      "\t 23 | 00:36:12 (207) |  0.01 | 128 | 60% |  500 |   2.30% |  1.72% |  500 |  2.30% |  200\n",
      "\t 24 | 00:39:15 (182) |  0.10 | 128 | 60% |  500 |   3.26% |  1.51% |  400 |  1.44% |  450\n",
      "\t 25 | 00:40:44 ( 89) |  0.30 |  32 | 30% | 1000 |   3.76% |  1.64% |  100 |  1.90% |  450\n",
      "\t 26 | 00:43:50 (187) |  0.10 | 128 | 95% | 1000 |   3.87% |  1.40% |  500 |  1.63% |  300\n",
      "\t 27 | 00:45:31 (101) |  0.01 |  32 | 60% |  500 |   2.47% |  1.50% |  450 |  1.44% |  400\n",
      "\t 28 | 00:46:21 ( 50) |  0.10 |   4 | 95% | 1000 |   3.04% |  1.60% |  350 |  2.04% |  400\n",
      "\t 29 | 00:49:25 (184) |  0.30 | 128 | 95% | 1000 |   4.34% |  1.52% |   50 |  1.58% |   25\n",
      "\t 30 | 00:50:14 ( 49) |  0.30 |   4 | 95% |  250 |   4.07% |  1.90% |  250 |  1.93% |  150\n",
      "\t 31 | 00:51:14 ( 60) |  0.01 |   8 | 95% |  250 |   2.67% |  1.70% |  500 |  1.67% |  300\n",
      "\t 32 | 00:54:35 (201) |  0.01 | 128 | 60% |  250 |   1.92% |  1.53% |  500 |  1.95% |  400\n",
      "\t 33 | 00:57:36 (181) |  0.30 | 128 | 60% | 1000 |   3.76% |  1.67% |   25 |  1.62% |  500\n",
      "\t 34 | 00:58:39 ( 63) |  0.01 |   8 | 30% |  250 |   0.93% |  1.22% |  350 |  1.07% |  450\n",
      "\t 35 | 01:02:04 (205) |  0.01 | 128 | 95% |  250 |   3.51% |  1.88% |  450 |  1.58% |  450\n",
      "\t 36 | 01:03:32 ( 88) |  0.10 |  32 | 95% | 1000 |   4.64% |  1.95% |   25 |  1.84% |   25\n",
      "\t 37 | 01:06:41 (189) |  0.10 | 128 | 30% | 1000 |   3.32% |  1.39% |   25 |  1.23% |  450\n",
      "\t 38 | 01:10:03 (201) |  0.01 | 128 | 30% |  500 |   2.51% |  1.75% |  250 |  1.63% |  400\n",
      "\t 39 | 01:10:53 ( 50) |  0.10 |   4 | 95% |  250 |   3.16% |  1.49% |  400 |  1.43% |   25\n",
      "\t 40 | 01:11:45 ( 53) |  0.01 |   4 | 60% | 1000 |   1.62% |  0.91% |   50 |  0.83% |  200\n",
      "\t 41 | 01:13:17 ( 92) |  0.10 |  32 | 30% |  500 |   2.68% |  1.92% |  200 |  1.73% |  200\n",
      "\t 42 | 01:16:16 (179) |  0.10 | 128 | 30% |  500 |   2.92% |  1.46% |  150 |  1.81% |  400\n",
      "\t 43 | 01:17:06 ( 50) |  0.10 |   4 | 60% |  250 |   3.29% |  1.78% |  500 |  1.76% |  500\n",
      "\t 44 | 01:20:39 (213) |  0.01 | 128 | 95% | 1000 |   2.92% |  1.93% |  500 |  2.17% |  500\n",
      "\t 45 | 01:21:41 ( 61) |  0.01 |   8 | 95% |  500 |   2.79% |  1.67% |  500 |  1.65% |  300\n",
      "\t 46 | 01:24:39 (178) |  0.30 | 128 | 30% | 1000 |   2.90% |  1.52% |   25 |  1.82% |  300\n",
      "\t 47 | 01:28:07 (208) |  0.01 | 128 | 95% |  500 |   3.65% |  2.07% |  500 |  1.93% |  500\n",
      "\t 48 | 01:29:42 ( 95) |  0.10 |  32 | 30% | 1000 |   2.96% |  1.63% |   25 |  2.03% |  400\n",
      "\t 49 | 01:30:34 ( 52) |  0.01 |   4 | 95% |  500 |   2.05% |  0.90% |  200 |  1.35% |  200\n",
      "\t 50 | 01:31:25 ( 51) |  0.10 |   4 | 95% |  500 |   2.96% |  1.63% |  400 |  1.83% |  450\n",
      "\t 51 | 01:34:31 (187) |  0.10 | 128 | 60% | 1000 |   3.03% |  1.32% |  150 |  1.53% |  100\n",
      "\t 52 | 01:35:28 ( 57) |  0.30 |   8 | 60% |  500 |   3.19% |  1.37% |  100 |  1.48% |  300\n",
      "\t 53 | 01:38:45 (197) |  0.01 | 128 | 30% |  250 |   2.31% |  1.42% |   25 |  2.10% |  250\n",
      "Lookahead:  1 | Train: 252 | Test: 63 | Params:  54 | Train configs: 6\n",
      "\t  0 | 00:00:32 ( 32) |  0.01 |   8 | 95% |  250 |  -0.31% |  0.83% |   10 |  1.38% |  400\n",
      "\t  1 | 00:01:03 ( 31) |  0.01 |   8 | 30% |  250 |   0.09% |  0.11% |  500 |  0.86% |  350\n",
      "\t  2 | 00:03:14 (131) |  0.30 | 128 | 30% |  500 |   1.86% |  1.03% |  400 |  1.13% |  100\n",
      "\t  3 | 00:03:43 ( 29) |  0.30 |   8 | 30% |  250 |   1.26% |  1.10% |  300 |  1.68% |  200\n",
      "\t  4 | 00:06:14 (151) |  0.10 | 128 | 60% | 1000 |   0.81% |  1.42% |  350 |  1.64% |  250\n",
      "\t  5 | 00:07:11 ( 56) |  0.30 |  32 | 60% | 1000 |   2.10% |  1.09% |  350 |  1.56% |  250\n",
      "\t  6 | 00:08:08 ( 58) |  0.10 |  32 | 60% | 1000 |   1.08% |  1.31% |  400 |  1.50% |  300\n",
      "\t  7 | 00:08:40 ( 32) |  0.01 |   8 | 95% | 1000 |  -0.15% |  0.79% |   10 |  1.17% |  300\n",
      "\t  8 | 00:09:11 ( 31) |  0.10 |   8 | 60% |  500 |   0.55% |  1.04% |  450 |  1.04% |  500\n",
      "\t  9 | 00:09:36 ( 25) |  0.10 |   4 | 30% |  500 |   0.39% |  0.86% |  400 |  1.45% |  300\n",
      "\t 10 | 00:11:36 (120) |  0.30 | 128 | 30% |  250 |   2.27% |  0.88% |  200 |  0.66% |   10\n",
      "\t 11 | 00:12:35 ( 59) |  0.01 |  32 | 30% | 1000 |   0.91% |  0.86% |  450 |  1.50% |  500\n",
      "\t 12 | 00:14:52 (137) |  0.10 | 128 | 30% | 1000 |   1.61% |  1.21% |  500 |  1.15% |  150\n",
      "\t 13 | 00:15:22 ( 30) |  0.10 |   8 | 60% |  250 |   0.88% |  0.70% |  500 |  1.12% |  100\n",
      "\t 14 | 00:15:46 ( 24) |  0.30 |   4 | 30% |  500 |   1.05% |  0.81% |  500 |  1.23% |   75\n",
      "\t 15 | 00:16:38 ( 52) |  0.30 |  32 | 30% |  500 |   1.92% |  1.32% |  350 |  1.52% |  350\n",
      "\t 16 | 00:17:35 ( 57) |  0.01 |  32 | 30% |  250 |   0.54% |  0.72% |  450 |  1.44% |  300\n",
      "\t 17 | 00:18:05 ( 30) |  0.30 |   8 | 30% |  500 |   1.25% |  0.88% |   50 |  1.00% |   50\n",
      "\t 18 | 00:18:34 ( 30) |  0.30 |   8 | 60% | 1000 |   0.56% |  1.15% |   75 |  1.33% |  500\n",
      "\t 19 | 00:18:59 ( 24) |  0.10 |   4 | 60% |  250 |   0.36% |  1.02% |   50 |  1.27% |  300\n",
      "\t 20 | 00:19:51 ( 52) |  0.10 |  32 | 95% |  250 |  -0.06% |  1.38% |  300 |  1.33% |  450\n",
      "\t 21 | 00:22:09 (138) |  0.30 | 128 | 30% | 1000 |   1.92% |  1.25% |   75 |  1.20% |  350\n",
      "\t 22 | 00:23:06 ( 57) |  0.30 |  32 | 95% | 1000 |   0.19% |  1.48% |  200 |  1.94% |  100\n",
      "\t 23 | 00:25:21 (135) |  0.01 | 128 | 60% |  250 |   0.32% |  1.03% |  500 |  1.60% |  150\n",
      "\t 24 | 00:26:19 ( 58) |  0.01 |  32 | 95% |  500 |   0.35% |  0.65% |   25 |  1.15% |  350\n",
      "\t 25 | 00:26:50 ( 31) |  0.01 |   8 | 95% |  500 |  -0.31% |  0.76% |   10 |  0.88% |  450\n",
      "\t 26 | 00:28:55 (125) |  0.10 | 128 | 30% |  250 |   1.78% |  1.04% |  500 |  1.50% |  150\n",
      "\t 27 | 00:29:24 ( 29) |  0.30 |   8 | 95% |  500 |   1.02% |  1.09% |   75 |  1.31% |  300\n",
      "\t 28 | 00:30:17 ( 53) |  0.30 |  32 | 95% |  500 |   0.34% |  0.99% |  300 |  1.14% |  300\n",
      "\t 29 | 00:30:42 ( 25) |  0.30 |   4 | 30% | 1000 |   1.26% |  1.04% |  500 |  1.46% |  450\n",
      "\t 30 | 00:33:05 (143) |  0.10 | 128 | 60% |  500 |   0.63% |  1.34% |  500 |  1.31% |  250\n",
      "\t 31 | 00:35:35 (150) |  0.10 | 128 | 95% |  500 |   0.49% |  1.46% |  400 |  1.43% |   10\n",
      "\t 32 | 00:36:35 ( 60) |  0.01 |  32 | 60% | 1000 |   1.07% |  0.86% |  450 |  1.71% |  150\n",
      "\t 33 | 00:37:31 ( 55) |  0.30 |  32 | 30% | 1000 |   1.60% |  1.13% |  400 |  1.42% |   75\n",
      "\t 34 | 00:37:55 ( 25) |  0.30 |   4 | 95% |  500 |   1.34% |  1.31% |  100 |  1.85% |  500\n",
      "\t 35 | 00:40:09 (134) |  0.10 | 128 | 95% |  250 |  -0.14% |  1.15% |  450 |  1.54% |  400\n",
      "\t 36 | 00:40:33 ( 25) |  0.01 |   4 | 95% |  500 |  -0.41% |  1.02% |  500 |  1.48% |  400\n",
      "\t 37 | 00:41:27 ( 54) |  0.10 |  32 | 30% |  500 |   1.57% |  1.14% |  400 |  1.60% |  150\n",
      "\t 38 | 00:41:58 ( 30) |  0.10 |   8 | 30% |  250 |   1.32% |  0.89% |  400 |  1.44% |  300\n",
      "\t 39 | 00:42:26 ( 29) |  0.30 |   8 | 60% |  250 |   0.90% |  1.09% |  500 |  1.04% |  350\n",
      "\t 40 | 00:44:47 (141) |  0.30 | 128 | 60% |  500 |   0.65% |  1.25% |  300 |  1.49% |  300\n",
      "\t 41 | 00:46:58 (130) |  0.10 | 128 | 60% |  250 |   1.71% |  1.01% |  100 |  1.08% |  300\n",
      "\t 42 | 00:47:23 ( 25) |  0.30 |   4 | 95% | 1000 |   1.80% |  1.62% |   10 |  1.94% |   10\n",
      "\t 43 | 00:47:53 ( 30) |  0.30 |   8 | 30% | 1000 |   1.73% |  0.99% |  350 |  1.42% |   75\n",
      "\t 44 | 00:48:43 ( 50) |  0.30 |  32 | 95% |  250 |  -0.76% |  1.08% |  200 |  1.59% |  250\n",
      "\t 45 | 00:49:08 ( 25) |  0.10 |   4 | 30% | 1000 |   0.76% |  0.84% |  500 |  1.16% |  450\n",
      "\t 46 | 00:50:06 ( 58) |  0.01 |  32 | 95% |  250 |  -0.18% |  0.67% |  400 |  0.88% |  500\n",
      "\t 47 | 00:50:37 ( 31) |  0.10 |   8 | 95% | 1000 |   0.08% |  1.18% |  500 |  1.82% |   25\n",
      "\t 48 | 00:51:09 ( 32) |  0.01 |   8 | 30% |  500 |   0.11% |  0.35% |  450 |  0.97% |   25\n",
      "\t 49 | 00:53:17 (129) |  0.01 | 128 | 30% | 1000 |   1.11% |  1.00% |  500 |  1.38% |  100\n",
      "\t 50 | 00:54:16 ( 58) |  0.01 |  32 | 30% |  500 |   0.77% |  0.75% |  450 |  1.64% |  350\n",
      "\t 51 | 00:55:07 ( 52) |  0.10 |  32 | 60% |  250 |   0.23% |  1.18% |  500 |  1.21% |  400\n",
      "\t 52 | 00:55:57 ( 49) |  0.30 |  32 | 30% |  250 |   1.67% |  0.94% |  450 |  1.01% |  450\n",
      "\t 53 | 00:56:49 ( 52) |  0.10 |  32 | 30% |  250 |   1.29% |  0.68% |  450 |  1.25% |  250\n",
      "Lookahead: 21 | Train: 252 | Test: 63 | Params:  54 | Train configs: 6\n",
      "\t  0 | 00:02:09 (129) |  0.30 | 128 | 30% |  500 |   5.33% |  4.80% |  100 |  4.58% |   75\n",
      "\t  1 | 00:04:19 (131) |  0.01 | 128 | 30% |  250 |   2.81% |  3.90% |  500 |  3.66% |  450\n",
      "\t  2 | 00:06:42 (142) |  0.01 | 128 | 95% |  250 |   3.79% |  4.47% |  500 |  4.33% |  450\n",
      "\t  3 | 00:07:13 ( 32) |  0.01 |   8 | 60% |  250 |   4.73% |  4.12% |  250 |  4.07% |  250\n",
      "\t  4 | 00:07:42 ( 29) |  0.30 |   8 | 60% |  250 |   6.91% |  4.20% |  450 |  4.77% |  400\n",
      "\t  5 | 00:09:41 (119) |  0.30 | 128 | 60% |  250 |   4.92% |  4.57% |  300 |  4.72% |  100\n",
      "\t  6 | 00:11:50 (129) |  0.01 | 128 | 30% | 1000 |   3.73% |  4.86% |  500 |  5.06% |  500\n",
      "\t  7 | 00:12:22 ( 32) |  0.01 |   8 | 30% |  250 |   4.64% |  3.23% |  500 |  2.60% |  500\n",
      "\t  8 | 00:13:18 ( 56) |  0.30 |  32 | 95% | 1000 |   5.17% |  4.55% |  100 |  5.00% |  350\n",
      "\t  9 | 00:13:44 ( 26) |  0.01 |   4 | 60% | 1000 |   3.86% |  4.04% |  500 |  3.12% |  500\n",
      "\t 10 | 00:14:08 ( 24) |  0.30 |   4 | 60% | 1000 |   7.49% |  4.74% |  450 |  5.43% |  450\n",
      "\t 11 | 00:16:41 (153) |  0.30 | 128 | 95% | 1000 |   4.54% |  5.19% |   50 |  4.90% |  350\n",
      "\t 12 | 00:18:54 (133) |  0.10 | 128 | 30% |  500 |   6.08% |  4.82% |  500 |  4.55% |  500\n",
      "\t 13 | 00:19:27 ( 33) |  0.01 |   8 | 60% | 1000 |   4.62% |  4.34% |  250 |  4.37% |  250\n",
      "\t 14 | 00:20:27 ( 60) |  0.01 |  32 | 30% | 1000 |   3.73% |  4.29% |  500 |  4.64% |  500\n",
      "\t 15 | 00:21:27 ( 61) |  0.01 |  32 | 95% | 1000 |   3.72% |  3.68% |  500 |  3.98% |  500\n",
      "\t 16 | 00:21:57 ( 29) |  0.30 |   8 | 30% |  250 |   8.05% |  5.06% |  450 |  4.96% |  400\n",
      "\t 17 | 00:22:22 ( 25) |  0.10 |   4 | 95% |  250 |   5.60% |  3.97% |   10 |  5.17% |   10\n",
      "\t 18 | 00:23:21 ( 59) |  0.01 |  32 | 95% |  500 |   3.75% |  3.51% |  500 |  4.52% |  450\n",
      "\t 19 | 00:24:16 ( 55) |  0.30 |  32 | 60% | 1000 |   6.65% |  5.01% |  400 |  6.11% |  400\n",
      "\t 20 | 00:24:46 ( 30) |  0.30 |   8 | 95% |  500 |   7.30% |  3.83% |  450 |  4.25% |  400\n",
      "\t 21 | 00:25:11 ( 25) |  0.10 |   4 | 95% | 1000 |   6.36% |  4.32% |   10 |  5.44% |   10\n",
      "\t 22 | 00:26:05 ( 55) |  0.10 |  32 | 95% |  500 |   4.75% |  4.17% |  300 |  4.79% |  200\n",
      "\t 23 | 00:26:31 ( 25) |  0.01 |   4 | 60% |  250 |   3.84% |  4.04% |  500 |  3.11% |  400\n",
      "\t 24 | 00:28:29 (119) |  0.30 | 128 | 30% |  250 |   5.85% |  4.75% |  250 |  4.68% |  250\n",
      "\t 25 | 00:28:54 ( 24) |  0.30 |   4 | 95% |  250 |   7.83% |  4.14% |  500 |  5.12% |  250\n",
      "\t 26 | 00:30:60 (126) |  0.10 | 128 | 30% |  250 |   6.46% |  5.19% |  450 |  4.82% |  500\n",
      "\t 27 | 00:31:58 ( 58) |  0.01 |  32 | 60% |  250 |   3.42% |  3.42% |  500 |  4.10% |  500\n",
      "\t 28 | 00:32:56 ( 58) |  0.10 |  32 | 95% | 1000 |   5.10% |  4.59% |  500 |  4.86% |  500\n",
      "\t 29 | 00:33:27 ( 31) |  0.10 |   8 | 30% |  250 |   7.39% |  3.72% |  450 |  3.42% |  450\n",
      "\t 30 | 00:33:59 ( 32) |  0.01 |   8 | 60% |  500 |   4.76% |  4.01% |  250 |  3.90% |  350\n",
      "\t 31 | 00:36:32 (152) |  0.10 | 128 | 95% | 1000 |   4.77% |  5.47% |  200 |  5.39% |  200\n",
      "\t 32 | 00:37:04 ( 33) |  0.01 |   8 | 95% | 1000 |   3.04% |  3.51% |  400 |  3.89% |  200\n",
      "\t 33 | 00:39:15 (131) |  0.01 | 128 | 30% |  500 |   3.51% |  4.43% |  500 |  4.50% |  450\n",
      "\t 34 | 00:40:07 ( 53) |  0.10 |  32 | 60% |  250 |   5.87% |  5.11% |  400 |  5.50% |  500\n",
      "\t 35 | 00:40:39 ( 32) |  0.10 |   8 | 95% | 1000 |   5.05% |  4.07% |  500 |  4.88% |  400\n",
      "\t 36 | 00:41:38 ( 59) |  0.01 |  32 | 95% |  250 |   3.53% |  3.30% |  500 |  4.22% |  450\n",
      "\t 37 | 00:42:09 ( 32) |  0.10 |   8 | 95% |  500 |   6.91% |  4.40% |  500 |  5.39% |  450\n",
      "\t 38 | 00:42:41 ( 31) |  0.10 |   8 | 60% | 1000 |   7.89% |  4.72% |  350 |  5.67% |  400\n",
      "\t 39 | 00:43:05 ( 24) |  0.30 |   4 | 60% |  250 |   6.56% |  4.97% |  350 |  5.74% |  450\n",
      "\t 40 | 00:43:57 ( 52) |  0.30 |  32 | 95% |  500 |   5.80% |  4.36% |  400 |  5.00% |  400\n",
      "\t 41 | 00:44:28 ( 30) |  0.30 |   8 | 30% | 1000 |   8.28% |  5.28% |  300 |  5.91% |  500\n",
      "\t 42 | 00:44:53 ( 25) |  0.01 |   4 | 95% |  500 |   1.96% |  4.22% |  100 |  4.04% |  150\n",
      "\t 43 | 00:45:42 ( 49) |  0.30 |  32 | 30% |  250 |   8.11% |  5.19% |  400 |  5.62% |  350\n",
      "\t 44 | 00:46:13 ( 31) |  0.10 |   8 | 30% |  500 |   8.34% |  4.07% |  500 |  3.64% |  500\n",
      "\t 45 | 00:46:38 ( 24) |  0.30 |   4 | 60% |  500 |   7.10% |  5.30% |  450 |  5.62% |  450\n",
      "\t 46 | 00:47:27 ( 50) |  0.30 |  32 | 95% |  250 |   6.32% |  3.69% |  350 |  4.01% |  400\n",
      "\t 47 | 00:47:57 ( 30) |  0.30 |   8 | 95% |  250 |   6.12% |  4.13% |  450 |  4.75% |  400\n",
      "\t 48 | 00:48:27 ( 30) |  0.30 |   8 | 60% | 1000 |   7.18% |  4.27% |  450 |  4.51% |  450\n",
      "\t 49 | 00:50:42 (135) |  0.10 | 128 | 30% | 1000 |   6.63% |  5.63% |  450 |  5.26% |  500\n",
      "\t 50 | 00:51:35 ( 53) |  0.10 |  32 | 95% |  250 |   4.69% |  4.17% |  500 |  5.06% |  500\n",
      "\t 51 | 00:51:60 ( 25) |  0.30 |   4 | 95% | 1000 |   8.15% |  4.89% |  400 |  5.77% |  400\n",
      "\t 52 | 00:54:21 (141) |  0.01 | 128 | 95% |  500 |   3.73% |  5.02% |  400 |  4.94% |  500\n",
      "\t 53 | 00:54:46 ( 25) |  0.10 |   4 | 60% |  500 |   7.97% |  4.90% |   25 |  4.79% |  400\n",
      "Lookahead:  5 | Train: 1134 | Test: 63 | Params:  54 | Train configs: 6\n",
      "\t  0 | 00:00:52 ( 52) |  0.10 |   4 | 60% |  500 |   8.76% |  3.36% |  400 |  3.31% |  400\n",
      "\t  1 | 00:01:52 ( 59) |  0.10 |   8 | 95% |  250 |   9.85% |  3.32% |   50 |  2.84% |   50\n",
      "\t  2 | 00:03:18 ( 86) |  0.30 |  32 | 30% |  500 |   8.53% |  3.43% |   25 |  3.72% |   25\n",
      "\t  3 | 00:06:27 (190) |  0.10 | 128 | 95% | 1000 |   6.94% |  1.75% |   50 |  1.53% |   50\n",
      "\t  4 | 00:07:20 ( 53) |  0.01 |   4 | 30% | 1000 |   5.15% |  2.71% |  500 |  2.59% |  450\n",
      "\t  5 | 00:08:18 ( 58) |  0.30 |   8 | 30% |  250 |   8.71% |  3.45% |   25 |  3.82% |   25\n",
      "\t  6 | 00:09:53 ( 94) |  0.10 |  32 | 30% |  250 |   7.08% |  3.59% |   25 |  4.15% |   25\n",
      "\t  7 | 00:12:52 (180) |  0.30 | 128 | 60% | 1000 |   7.82% |  2.69% |   10 |  1.94% |   50\n",
      "\t  8 | 00:14:16 ( 84) |  0.30 |  32 | 60% |  500 |   7.34% |  2.15% |   50 |  1.59% |   10\n",
      "\t  9 | 00:15:42 ( 86) |  0.30 |  32 | 95% | 1000 |  10.22% |  3.09% |   25 |  2.36% |  100\n",
      "\t 10 | 00:18:25 (163) |  0.30 | 128 | 95% |  250 |   6.89% |  2.05% |   10 |  1.75% |  100\n",
      "\t 11 | 00:20:03 ( 97) |  0.10 |  32 | 30% | 1000 |   6.69% |  3.50% |   25 |  3.89% |   25\n",
      "\t 12 | 00:21:33 ( 91) |  0.10 |  32 | 95% |  500 |   9.25% |  2.85% |   75 |  2.55% |  400\n",
      "\t 13 | 00:23:09 ( 96) |  0.10 |  32 | 30% |  500 |   7.28% |  3.44% |   25 |  3.65% |   25\n",
      "\t 14 | 00:23:60 ( 51) |  0.30 |   4 | 60% |  500 |   9.99% |  3.60% |   50 |  3.55% |   50\n",
      "\t 15 | 00:24:51 ( 51) |  0.30 |   4 | 30% |  500 |   8.24% |  4.24% |  350 |  3.96% |  350\n",
      "\t 16 | 00:26:24 ( 93) |  0.10 |  32 | 60% |  500 |   9.56% |  3.46% |   75 |  3.32% |  100\n",
      "\t 17 | 00:27:18 ( 54) |  0.01 |   4 | 60% |  500 |   4.97% |  2.55% |  500 |  2.03% |  350\n",
      "\t 18 | 00:28:25 ( 67) |  0.01 |   8 | 30% |  500 |   5.79% |  2.67% |  300 |  2.94% |  300\n",
      "\t 19 | 00:29:24 ( 59) |  0.30 |   8 | 60% |  500 |  10.29% |  3.53% |  150 |  3.01% |  200\n",
      "\t 20 | 00:30:23 ( 59) |  0.30 |   8 | 60% | 1000 |  10.06% |  2.86% |   75 |  2.31% |   75\n",
      "\t 21 | 00:31:29 ( 67) |  0.01 |   8 | 30% |  250 |   5.86% |  2.72% |  300 |  2.77% |  300\n",
      "\t 22 | 00:32:28 ( 59) |  0.30 |   8 | 95% | 1000 |  11.89% |  3.37% |  100 |  3.75% |  100\n",
      "\t 23 | 00:33:20 ( 52) |  0.30 |   4 | 95% | 1000 |  10.20% |  3.07% |   50 |  2.84% |   50\n",
      "\t 24 | 00:34:12 ( 51) |  0.30 |   4 | 30% |  250 |   8.32% |  3.72% |  150 |  3.70% |  100\n",
      "\t 25 | 00:35:10 ( 59) |  0.30 |   8 | 95% |  500 |  10.17% |  3.46% |   75 |  3.09% |   75\n",
      "\t 26 | 00:36:10 ( 59) |  0.30 |   8 | 30% |  500 |   8.71% |  3.44% |  150 |  3.56% |   75\n",
      "\t 27 | 00:37:12 ( 62) |  0.10 |   8 | 60% | 1000 |   9.55% |  3.69% |  250 |  3.29% |  400\n",
      "\t 28 | 00:38:40 ( 87) |  0.30 |  32 | 60% | 1000 |   8.17% |  2.73% |   50 |  1.81% |   50\n",
      "\t 29 | 00:40:29 (110) |  0.01 |  32 | 60% |  250 |   6.66% |  2.00% |  500 |  1.91% |   10\n",
      "\t 30 | 00:44:01 (212) |  0.01 | 128 | 60% |  250 |   4.63% |  1.67% |  500 |  1.51% |  450\n",
      "\t 31 | 00:44:53 ( 52) |  0.10 |   4 | 30% |  500 |   6.95% |  3.78% |  450 |  3.80% |  350\n",
      "\t 32 | 00:47:49 (177) |  0.10 | 128 | 95% |  250 |   6.33% |  1.26% |  200 |  0.95% |  400\n",
      "\t 33 | 00:50:51 (181) |  0.10 | 128 | 30% |  500 |   7.06% |  3.34% |   25 |  3.44% |   25\n",
      "\t 34 | 00:54:30 (219) |  0.01 | 128 | 95% |  250 |   5.38% |  1.22% |  500 |  1.39% |  500\n",
      "\t 35 | 00:57:28 (179) |  0.30 | 128 | 30% | 1000 |   6.98% |  3.05% |   10 |  2.80% |   10\n",
      "\t 36 | 00:58:35 ( 67) |  0.01 |   8 | 60% | 1000 |   6.70% |  2.74% |  500 |  1.73% |  400\n",
      "\t 37 | 01:01:35 (180) |  0.10 | 128 | 60% |  500 |   6.28% |  1.68% |   50 |  1.51% |   50\n",
      "\t 38 | 01:02:40 ( 66) |  0.01 |   8 | 30% | 1000 |   5.69% |  2.76% |  300 |  2.82% |  400\n",
      "\t 39 | 01:03:48 ( 68) |  0.01 |   8 | 60% |  500 |   6.83% |  2.80% |  500 |  2.06% |  450\n",
      "\t 40 | 01:04:55 ( 66) |  0.01 |   8 | 95% |  250 |   7.42% |  3.13% |  500 |  2.04% |  400\n",
      "\t 41 | 01:05:48 ( 53) |  0.01 |   4 | 95% |  500 |   5.69% |  2.41% |  150 |  1.65% |   10\n",
      "\t 42 | 01:06:41 ( 54) |  0.01 |   4 | 30% |  250 |   4.97% |  2.60% |  500 |  2.84% |  450\n",
      "\t 43 | 01:08:29 (107) |  0.01 |  32 | 30% |  500 |   5.40% |  3.02% |   25 |  2.96% |  200\n",
      "\t 44 | 01:09:21 ( 53) |  0.10 |   4 | 95% | 1000 |  10.26% |  3.29% |  400 |  2.91% |  500\n",
      "\t 45 | 01:13:03 (221) |  0.01 | 128 | 95% | 1000 |   4.86% |  1.37% |  500 |  1.16% |  450\n",
      "\t 46 | 01:13:55 ( 52) |  0.01 |   4 | 95% |  250 |   5.49% |  2.40% |  150 |  1.65% |   10\n",
      "\t 47 | 01:15:03 ( 67) |  0.01 |   8 | 60% |  250 |   6.73% |  2.76% |  500 |  2.03% |  500\n",
      "\t 48 | 01:18:04 (182) |  0.10 | 128 | 95% |  500 |   6.14% |  0.83% |   75 |  0.79% |   50\n",
      "\t 49 | 01:18:54 ( 50) |  0.30 |   4 | 60% |  250 |   9.05% |  3.43% |   75 |  3.35% |   50\n",
      "\t 50 | 01:19:55 ( 61) |  0.10 |   8 | 60% |  500 |   9.87% |  3.82% |  300 |  3.42% |  450\n",
      "\t 51 | 01:21:16 ( 81) |  0.30 |  32 | 60% |  250 |   7.49% |  2.11% |   10 |  1.53% |   75\n",
      "\t 52 | 01:22:45 ( 89) |  0.30 |  32 | 30% | 1000 |   8.03% |  3.11% |   25 |  2.71% |   25\n",
      "\t 53 | 01:23:36 ( 52) |  0.10 |   4 | 60% | 1000 |   8.03% |  3.75% |  350 |  3.73% |  250\n",
      "Lookahead: 21 | Train: 1134 | Test: 63 | Params:  54 | Train configs: 6\n",
      "\t  0 | 00:00:59 ( 59) |  0.30 |   8 | 30% |  250 |   4.77% |  8.24% |   25 |  8.86% |   10\n",
      "\t  1 | 00:04:06 (188) |  0.10 | 128 | 30% | 1000 |   5.31% |  6.69% |   10 |  6.95% |  500\n",
      "\t  2 | 00:05:00 ( 54) |  0.01 |   4 | 60% | 1000 |   6.72% |  7.69% |  500 |  6.86% |  500\n",
      "\t  3 | 00:07:45 (165) |  0.30 | 128 | 95% |  500 |   1.54% |  3.53% |   50 |  5.27% |   50\n",
      "\t  4 | 00:08:53 ( 68) |  0.01 |   8 | 60% | 1000 |   5.79% |  6.78% |  450 |  6.04% |  150\n",
      "\t  5 | 00:09:46 ( 52) |  0.10 |   4 | 60% |  500 |   7.10% |  7.81% |   50 |  7.68% |  200\n",
      "\t  6 | 00:10:38 ( 52) |  0.30 |   4 | 30% |  250 |   6.72% |  8.32% |   50 |  9.43% |  100\n",
      "\t  7 | 00:11:40 ( 62) |  0.10 |   8 | 95% |  500 |   5.65% |  5.76% |   75 |  5.90% |  350\n",
      "\t  8 | 00:12:33 ( 53) |  0.10 |   4 | 95% |  250 |   7.31% |  7.31% |  100 |  6.59% |  200\n",
      "\t  9 | 00:13:34 ( 62) |  0.10 |   8 | 60% |  250 |   6.24% |  7.09% |  100 |  7.11% |  100\n",
      "\t 10 | 00:15:23 (109) |  0.01 |  32 | 30% | 1000 |   4.70% |  7.14% |  150 |  8.04% |  250\n",
      "\t 11 | 00:16:27 ( 63) |  0.10 |   8 | 30% |  250 |   6.29% |  8.02% |  100 |  9.19% |  100\n",
      "\t 12 | 00:18:19 (112) |  0.01 |  32 | 95% |  250 |   6.34% |  6.74% |  500 |  6.98% |  500\n",
      "\t 13 | 00:19:17 ( 58) |  0.30 |   8 | 60% |  250 |   5.68% |  6.43% |   25 |  7.10% |  250\n",
      "\t 14 | 00:20:10 ( 53) |  0.10 |   4 | 95% |  500 |   8.49% |  7.10% |  150 |  6.66% |  150\n",
      "\t 15 | 00:21:45 ( 95) |  0.10 |  32 | 60% | 1000 |   5.00% |  6.44% |  400 |  8.14% |  300\n",
      "\t 16 | 00:22:40 ( 55) |  0.01 |   4 | 95% |  250 |   6.13% |  6.86% |  500 |  5.67% |  400\n",
      "\t 17 | 00:24:03 ( 84) |  0.30 |  32 | 60% |  500 |   5.30% |  5.25% |  150 |  6.63% |   75\n",
      "\t 18 | 00:24:55 ( 51) |  0.30 |   4 | 95% |  250 |   5.40% |  6.85% |   50 |  6.89% |   50\n",
      "\t 19 | 00:25:53 ( 58) |  0.30 |   8 | 60% |  500 |   5.91% |  6.18% |   25 |  7.43% |  500\n",
      "\t 20 | 00:26:48 ( 55) |  0.01 |   4 | 95% |  500 |   6.14% |  6.90% |  500 |  5.73% |  500\n",
      "\t 21 | 00:29:44 (176) |  0.30 | 128 | 60% | 1000 |   3.86% |  6.13% |   75 |  6.78% |   50\n",
      "\t 22 | 00:32:52 (188) |  0.10 | 128 | 95% | 1000 |   5.31% |  6.67% |   75 |  8.00% |   75\n",
      "\t 23 | 00:33:59 ( 67) |  0.01 |   8 | 95% | 1000 |   5.17% |  6.14% |  500 |  5.25% |  150\n",
      "\t 24 | 00:34:50 ( 51) |  0.30 |   4 | 95% |  500 |   5.39% |  7.14% |  100 |  8.54% |  450\n",
      "\t 25 | 00:37:47 (177) |  0.10 | 128 | 95% |  500 |   4.58% |  6.52% |  100 |  7.83% |  100\n",
      "\t 26 | 00:38:55 ( 68) |  0.01 |   8 | 60% |  500 |   5.57% |  6.70% |  450 |  5.89% |  250\n",
      "\t 27 | 00:40:26 ( 90) |  0.10 |  32 | 60% |  500 |   4.85% |  5.69% |  150 |  7.50% |  300\n",
      "\t 28 | 00:41:34 ( 68) |  0.01 |   8 | 60% |  250 |   5.99% |  6.76% |  500 |  6.11% |  150\n",
      "\t 29 | 00:42:26 ( 52) |  0.30 |   4 | 30% | 1000 |   7.28% |  8.45% |   25 |  9.46% |   50\n",
      "\t 30 | 00:43:20 ( 54) |  0.10 |   4 | 30% |  500 |   9.14% |  9.03% |  100 |  10.21% |  100\n",
      "\t 31 | 00:44:29 ( 69) |  0.01 |   8 | 30% |  500 |   7.30% |  7.52% |  500 |  7.85% |  500\n",
      "\t 32 | 00:47:26 (177) |  0.30 | 128 | 30% | 1000 |   4.27% |  6.02% |   10 |  6.29% |   50\n",
      "\t 33 | 00:50:22 (177) |  0.10 | 128 | 60% |  500 |   4.24% |  6.17% |   75 |  7.96% |   75\n",
      "\t 34 | 00:53:02 (160) |  0.30 | 128 | 30% |  250 |   5.15% |  6.35% |   10 |  6.49% |   25\n",
      "\t 35 | 00:53:57 ( 55) |  0.01 |   4 | 30% | 1000 |   9.47% |  7.86% |  500 |  7.32% |  500\n",
      "\t 36 | 00:54:49 ( 52) |  0.10 |   4 | 60% | 1000 |   7.58% |  7.90% |   50 |  7.38% |   75\n",
      "\t 37 | 00:55:42 ( 53) |  0.10 |   4 | 30% |  250 |   9.06% |  8.80% |  100 |  9.91% |  100\n",
      "\t 38 | 00:56:35 ( 53) |  0.10 |   4 | 60% |  250 |   7.52% |  7.96% |   75 |  7.59% |  100\n",
      "\t 39 | 00:57:28 ( 52) |  0.30 |   4 | 95% | 1000 |   5.50% |  6.36% |   75 |  7.98% |  400\n",
      "\t 40 | 01:01:04 (217) |  0.01 | 128 | 95% |  500 |   3.67% |  6.94% |  500 |  8.72% |  200\n",
      "\t 41 | 01:02:25 ( 81) |  0.30 |  32 | 60% |  250 |   5.56% |  6.02% |  450 |  7.21% |  450\n",
      "\t 42 | 01:03:27 ( 61) |  0.10 |   8 | 60% |  500 |   4.28% |  6.94% |   50 |  6.71% |  150\n",
      "\t 43 | 01:05:13 (107) |  0.01 |  32 | 30% |  500 |   5.33% |  7.14% |  200 |  8.14% |  300\n",
      "\t 44 | 01:07:06 (112) |  0.01 |  32 | 95% | 1000 |   5.85% |  6.73% |  500 |  6.92% |  400\n",
      "\t 45 | 01:08:41 ( 95) |  0.10 |  32 | 30% |  500 |   5.31% |  6.65% |   10 |  6.94% |  500\n",
      "\t 46 | 01:11:60 (199) |  0.01 | 128 | 30% |  250 |   4.19% |  6.75% |  150 |  7.87% |  250\n",
      "\t 47 | 01:12:54 ( 55) |  0.01 |   4 | 30% |  500 |   9.45% |  7.78% |  500 |  7.32% |  500\n",
      "\t 48 | 01:13:46 ( 51) |  0.30 |   4 | 60% |  500 |   4.72% |  7.28% |  100 |  8.18% |  300\n",
      "\t 49 | 01:15:37 (111) |  0.01 |  32 | 60% | 1000 |   6.02% |  6.53% |  500 |  7.56% |  300\n",
      "\t 50 | 01:18:31 (173) |  0.10 | 128 | 95% |  250 |   5.52% |  6.15% |  150 |  7.65% |  200\n",
      "\t 51 | 01:19:25 ( 54) |  0.01 |   4 | 60% |  250 |   6.78% |  7.74% |  500 |  7.13% |  500\n",
      "\t 52 | 01:20:17 ( 52) |  0.10 |   4 | 95% | 1000 |   7.83% |  7.42% |  150 |  7.12% |  250\n",
      "\t 53 | 01:21:17 ( 60) |  0.30 |   8 | 30% | 1000 |   6.81% |  8.20% |   25 |  8.73% |   25\n",
      "Lookahead:  5 | Train: 252 | Test: 63 | Params:  54 | Train configs: 6\n",
      "\t  0 | 00:00:32 ( 32) |  0.10 |   8 | 60% | 1000 |   2.21% |  0.94% |  450 |  2.08% |  450\n",
      "\t  1 | 00:00:56 ( 24) |  0.30 |   4 | 30% | 1000 |   2.83% |  1.12% |  500 |  2.02% |  400\n",
      "\t  2 | 00:01:26 ( 30) |  0.30 |   8 | 60% | 1000 |   3.32% |  1.62% |  350 |  1.95% |  500\n",
      "\t  3 | 00:03:25 (120) |  0.30 | 128 | 30% |  250 |   2.50% |  1.34% |  500 |  1.52% |  500\n",
      "\t  4 | 00:04:25 ( 59) |  0.01 |  32 | 30% | 1000 |   2.01% |  0.57% |   25 |  0.80% |  450\n",
      "\t  5 | 00:04:55 ( 30) |  0.30 |   8 | 95% | 1000 |   3.03% |  2.25% |  500 |  3.05% |  250\n",
      "\t  6 | 00:05:53 ( 58) |  0.10 |  32 | 60% | 1000 |   2.91% |  1.65% |  500 |  2.11% |  150\n",
      "\t  7 | 00:07:57 (124) |  0.30 | 128 | 60% |  250 |   2.64% |  1.59% |  400 |  1.86% |  250\n",
      "\t  8 | 00:10:09 (132) |  0.01 | 128 | 60% | 1000 |   2.47% |  1.42% |  150 |  1.79% |  200\n",
      "\t  9 | 00:10:58 ( 49) |  0.30 |  32 | 60% |  250 |   2.62% |  1.66% |  300 |  2.24% |  150\n",
      "\t 10 | 00:13:15 (137) |  0.01 | 128 | 60% |  500 |   2.41% |  1.25% |  250 |  1.70% |  500\n",
      "\t 11 | 00:14:06 ( 51) |  0.30 |  32 | 95% |  250 |   3.07% |  1.50% |  450 |  1.77% |  200\n",
      "\t 12 | 00:14:39 ( 33) |  0.01 |   8 | 60% | 1000 |   2.03% |  1.61% |   10 |  0.78% |   10\n",
      "\t 13 | 00:15:10 ( 31) |  0.10 |   8 | 60% |  500 |   2.73% |  1.60% |  250 |  3.01% |  500\n",
      "\t 14 | 00:15:35 ( 26) |  0.01 |   4 | 30% | 1000 |   1.17% |  0.28% |  500 | -0.33% |  500\n",
      "\t 15 | 00:18:13 (157) |  0.10 | 128 | 95% | 1000 |   2.63% |  1.78% |  250 |  1.87% |  400\n",
      "\t 16 | 00:18:44 ( 32) |  0.10 |   8 | 30% |  250 |   2.65% |  0.56% |  100 |  1.23% |  500\n",
      "\t 17 | 00:19:44 ( 59) |  0.01 |  32 | 95% |  500 |   2.61% |  1.76% |   75 |  1.67% |   75\n",
      "\t 18 | 00:20:15 ( 31) |  0.10 |   8 | 30% | 1000 |   2.60% |  0.50% |  100 |  0.96% |  450\n",
      "\t 19 | 00:22:38 (143) |  0.01 | 128 | 95% |  250 |   2.67% |  1.79% |  100 |  1.61% |  400\n",
      "\t 20 | 00:23:04 ( 26) |  0.01 |   4 | 60% | 1000 |   0.93% |  0.90% |   10 |  1.02% |   10\n",
      "\t 21 | 00:23:35 ( 32) |  0.10 |   8 | 95% | 1000 |   2.78% |  1.46% |  500 |  2.48% |  450\n",
      "\t 22 | 00:24:01 ( 25) |  0.01 |   4 | 60% |  250 |   0.92% |  1.28% |   10 |  0.94% |   10\n",
      "\t 23 | 00:26:14 (134) |  0.10 | 128 | 30% |  500 |   2.51% |  1.27% |  500 |  1.63% |  350\n",
      "\t 24 | 00:28:28 (133) |  0.10 | 128 | 95% |  250 |   2.55% |  1.89% |  400 |  2.48% |  400\n",
      "\t 25 | 00:28:52 ( 25) |  0.30 |   4 | 95% |  500 |   3.21% |  2.20% |  500 |  2.43% |  250\n",
      "\t 26 | 00:29:52 ( 59) |  0.10 |  32 | 95% | 1000 |   2.77% |  2.01% |  500 |  2.36% |  150\n",
      "\t 27 | 00:30:50 ( 58) |  0.01 |  32 | 30% |  500 |   1.93% |  0.49% |   25 |  1.00% |  500\n",
      "\t 28 | 00:31:49 ( 59) |  0.01 |  32 | 60% |  250 |   2.49% |  1.23% |  500 |  1.36% |  350\n",
      "\t 29 | 00:32:14 ( 25) |  0.01 |   4 | 30% |  250 |   1.16% |  0.10% |  500 | -0.35% |  500\n",
      "\t 30 | 00:32:39 ( 25) |  0.10 |   4 | 60% |  500 |   2.07% |  1.14% |   10 |  1.31% |  500\n",
      "\t 31 | 00:34:55 (136) |  0.10 | 128 | 30% | 1000 |   2.71% |  1.20% |  450 |  1.52% |  250\n",
      "\t 32 | 00:35:20 ( 25) |  0.01 |   4 | 95% |  500 |   0.99% |  3.96% |   25 |  3.90% |   25\n",
      "\t 33 | 00:35:44 ( 24) |  0.30 |   4 | 95% |  250 |   2.95% |  1.97% |  500 |  2.82% |  400\n",
      "\t 34 | 00:36:14 ( 30) |  0.30 |   8 | 95% |  250 |   2.75% |  2.23% |  400 |  2.67% |  500\n",
      "\t 35 | 00:37:08 ( 54) |  0.10 |  32 | 30% |  500 |   2.74% |  0.97% |  500 |  1.34% |  500\n",
      "\t 36 | 00:37:34 ( 25) |  0.01 |   4 | 30% |  500 |   1.18% |  0.12% |  500 | -0.60% |  500\n",
      "\t 37 | 00:38:34 ( 60) |  0.01 |  32 | 95% | 1000 |   2.59% |  1.54% |   75 |  1.53% |  150\n",
      "\t 38 | 00:40:52 (138) |  0.30 | 128 | 30% | 1000 |   2.82% |  1.27% |  350 |  1.79% |  300\n",
      "\t 39 | 00:41:17 ( 25) |  0.10 |   4 | 60% |  250 |   1.80% |  1.46% |   75 |  1.71% |  500\n",
      "\t 40 | 00:41:46 ( 29) |  0.30 |   8 | 60% |  250 |   3.76% |  1.78% |  350 |  2.39% |  250\n",
      "\t 41 | 00:42:10 ( 25) |  0.01 |   4 | 95% |  250 |   0.96% |  3.96% |   25 |  3.90% |   25\n",
      "\t 42 | 00:42:35 ( 25) |  0.10 |   4 | 30% |  500 |   2.08% |  0.65% |  250 |  1.28% |  400\n",
      "\t 43 | 00:43:25 ( 50) |  0.30 |  32 | 30% |  250 |   2.77% |  1.33% |  500 |  1.79% |  350\n",
      "\t 44 | 00:43:50 ( 25) |  0.10 |   4 | 95% |  500 |   2.25% |  1.59% |   50 |  1.74% |   50\n",
      "\t 45 | 00:44:22 ( 32) |  0.01 |   8 | 30% |  250 |   1.53% |  0.11% |  500 |  0.02% |  500\n",
      "\t 46 | 00:44:46 ( 24) |  0.30 |   4 | 60% |  500 |   3.29% |  1.69% |   10 |  2.30% |  350\n",
      "\t 47 | 00:45:11 ( 25) |  0.30 |   4 | 60% | 1000 |   3.15% |  1.66% |   10 |  2.13% |  450\n",
      "\t 48 | 00:45:42 ( 31) |  0.10 |   8 | 30% |  500 |   3.30% |  0.86% |  100 |  1.30% |  350\n",
      "\t 49 | 00:46:15 ( 33) |  0.01 |   8 | 95% | 1000 |   2.07% |  2.50% |   10 |  2.25% |   10\n",
      "\t 50 | 00:46:47 ( 32) |  0.01 |   8 | 30% |  500 |   1.55% |  0.08% |  450 |  0.41% |  450\n",
      "\t 51 | 00:47:12 ( 25) |  0.10 |   4 | 95% |  250 |   2.04% |  2.09% |   25 |  2.09% |   25\n",
      "\t 52 | 00:49:23 (131) |  0.01 | 128 | 30% |  500 |   2.22% |  0.66% |  500 |  1.49% |  450\n",
      "\t 53 | 00:51:33 (130) |  0.10 | 128 | 60% |  250 |   2.41% |  1.56% |  500 |  2.09% |  500\n"
     ]
    }
   ],
   "source": [
    "for lookahead, train_length, test_length in test_params:\n",
    "    # randomized grid search\n",
    "    cvp = np.random.choice(list(range(n_params)), size=int(n_params / 2), replace=False)\n",
    "    cv_params_ = [cv_params[i] for i in cvp]\n",
    "\n",
    "    # set up cross-validation\n",
    "    n_splits = int(2 * YEAR / test_length)\n",
    "    print(f'Lookahead: {lookahead:2.0f} | Train: {train_length:3.0f} | Test: {test_length:2.0f} | '\n",
    "          f'Params: {len(cv_params_):3.0f} | Train configs: {len(test_params)}')\n",
    "\n",
    "    # time-series cross-validation\n",
    "    cv = MultipleTimeSeriesCV(n_splits=n_splits, lookahead=lookahead, test_period_length=test_length,\n",
    "                              train_period_length=train_length)\n",
    "\n",
    "    label = label_dict[lookahead]\n",
    "    outcome_data = data.loc[:, features + [label]].dropna()\n",
    "\n",
    "    # binary dataset\n",
    "    lgb_data = lgb.Dataset(data=outcome_data.drop(label, axis=1), label=outcome_data[label],\n",
    "                           categorical_feature=categoricals, free_raw_data=False)\n",
    "    T = 0\n",
    "    predictions, metrics, feature_importance, daily_ic = [], [], [], []\n",
    "\n",
    "    # iterate over (shuffled) hyperparameter combinations\n",
    "    for p, param_vals in enumerate(cv_params_):\n",
    "        key = f'{lookahead}/{train_length}/{test_length}/' + '/'.join([str(p) for p in param_vals])\n",
    "        params = dict(zip(param_names, param_vals))\n",
    "        params.update(base_params)\n",
    "\n",
    "\n",
    "        start = time()\n",
    "        cv_preds, nrounds = [], []\n",
    "        ic_cv = defaultdict(list)\n",
    "\n",
    "        # iterate over folds\n",
    "        for i, (train_idx, test_idx) in enumerate(cv.split(X=outcome_data)):\n",
    "\n",
    "            # select train subset\n",
    "            lgb_train = lgb_data.subset(used_indices=train_idx.tolist(), params=params).construct()\n",
    "\n",
    "            # train model for num_boost_round\n",
    "            model = lgb.train(params=params, train_set=lgb_train, num_boost_round=num_boost_round,\n",
    "                              verbose_eval=False)\n",
    "            # log feature importance\n",
    "            if i == 0:\n",
    "                fi = get_fi(model).to_frame()\n",
    "            else:\n",
    "                fi[i] = get_fi(model)\n",
    "\n",
    "            # capture predictions\n",
    "            test_set = outcome_data.iloc[test_idx, :]\n",
    "            X_test = test_set.loc[:, model.feature_name()]\n",
    "            y_test = test_set.loc[:, label]\n",
    "            y_pred = {str(n): model.predict(X_test, num_iteration=n) for n in num_iterations}\n",
    "\n",
    "            # record predictions for each fold\n",
    "            cv_preds.append(y_test.to_frame('y_test').assign(**y_pred).assign(i=i))\n",
    "\n",
    "        # combine fold results\n",
    "        cv_preds = pd.concat(cv_preds).assign(**params)\n",
    "        predictions.append(cv_preds)\n",
    "\n",
    "        # compute IC per day\n",
    "        by_day = cv_preds.groupby(level='date')\n",
    "        ic_by_day = pd.concat([by_day.apply(lambda x: spearmanr(x.y_test, x[str(n)])[0]).to_frame(n)\n",
    "                               for n in num_iterations], axis=1)\n",
    "        daily_ic_mean = ic_by_day.mean()\n",
    "        daily_ic_mean_n = daily_ic_mean.idxmax()\n",
    "        daily_ic_median = ic_by_day.median()\n",
    "        daily_ic_median_n = daily_ic_median.idxmax()\n",
    "\n",
    "        # compute IC across all predictions\n",
    "        ic = [spearmanr(cv_preds.y_test, cv_preds[str(n)])[0] for n in num_iterations]\n",
    "        t = time() - start\n",
    "        T += t\n",
    "\n",
    "        # collect metrics\n",
    "        metrics = pd.Series(list(param_vals) + [t, daily_ic_mean.max(), daily_ic_mean_n, daily_ic_median.max(),\n",
    "                             daily_ic_median_n] + ic, index=metric_cols)\n",
    "        msg = f'\\t{p:3.0f} | {format_time(T)} ({t:3.0f}) | {params[\"learning_rate\"]:5.2f} | '\n",
    "        msg += f'{params[\"num_leaves\"]:3.0f} | {params[\"feature_fraction\"]:3.0%} | '\n",
    "        msg += f'{params[\"min_data_in_leaf\"]:4.0f} | '\n",
    "        msg += f' {max(ic):6.2%} | {ic_by_day.mean().max(): 6.2%} | {daily_ic_mean_n: 4.0f} | '\n",
    "        msg += f'{ic_by_day.median().max(): 6.2%} | {daily_ic_median_n: 4.0f}'\n",
    "        print(msg)\n",
    "\n",
    "        # persist results for given CV run and hyperparameter combination\n",
    "        metrics.to_hdf(lgb_store, 'metrics/' + key)\n",
    "        ic_by_day.assign(**params).to_hdf(lgb_store, 'daily_ic/' + key)\n",
    "        fi.T.describe().T.assign(**params).to_hdf(lgb_store, 'fi/' + key)\n",
    "        cv_preds.to_hdf(lgb_store, 'predictions/' + key)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "'5/252/63/0.1/128/0.6/250'"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# CatBoost Model Tuning\n",
    "\n",
    "# Hyperparameter Opts\n",
    "param_names = ['max_depth', 'min_child_samples']\n",
    "\n",
    "max_depth_opts = [3, 5, 7, 9]\n",
    "min_child_samples_opts = [20, 250, 500]\n",
    "cv_params = list(product(max_depth_opts, min_child_samples_opts))\n",
    "n_params = len(cv_params)\n",
    "\n",
    "# Train/Test Period Lengths\n",
    "lookaheads = [1, 5, 21]\n",
    "label_dict = dict(zip(lookaheads, labels))\n",
    "train_lengths = [int(4.5 * 252), 252]\n",
    "test_lengths = [63]\n",
    "test_params = list(product(lookaheads, train_lengths, test_lengths))\n",
    "\n",
    "# Custom Loss Function\n",
    "class CatBoostIC(object):\n",
    "    \"\"\"Custom IC eval metric for CatBoost\"\"\"\n",
    "\n",
    "    def is_max_optimal(self):\n",
    "        # Returns whether great values of metric are better\n",
    "        return True\n",
    "\n",
    "    def evaluate(self, approxes, target, weight):\n",
    "        target = np.array(target)\n",
    "        approxes = np.array(approxes).reshape(-1)\n",
    "        rho = spearmanr(approxes, target)[0]\n",
    "        return rho, 1\n",
    "\n",
    "    def get_final_error(self, error, weight):\n",
    "        # Returns final value of metric based on error and weight\n",
    "        return error\n",
    "\n",
    "# Run Cross-Validation\n",
    "cb_store = Path(results_path / 'tuning_catboost.h5')\n",
    "num_iterations = [10, 25, 50, 75] + list(range(100, 1001, 100))\n",
    "num_boost_round = num_iterations[-1]\n",
    "metric_cols = (param_names + ['t', 'daily_ic_mean', 'daily_ic_mean_n',\n",
    "                              'daily_ic_median', 'daily_ic_median_n'] + [str(n) for n in num_iterations])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "['max_depth',\n 'min_child_samples',\n 't',\n 'daily_ic_mean',\n 'daily_ic_mean_n',\n 'daily_ic_median',\n 'daily_ic_median_n',\n '10',\n '25',\n '50',\n '75',\n '100',\n '200',\n '300',\n '400',\n '500',\n '600',\n '700',\n '800',\n '900',\n '1000']"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_cols"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "[10, 25, 50, 75, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000]"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_iterations"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cvp: [ 2  6  9  3  1  4  5 10  7 11  0  8]\n",
      "cv_params_: [(3, 500), (7, 20), (9, 20), (5, 20), (3, 250), (5, 250), (5, 500), (9, 250), (7, 250), (9, 500), (3, 20), (7, 500)]\n",
      "\n",
      "Lookahead:  1 | Train: 1134 | Test: 63 | Params:  12 | Train configs: 6\n",
      "  0 | 00:04:59 (299) |   3 |  500 |  3.09% |  2.00% |  100 |  1.57% |  800\n",
      "  1 | 00:17:16 (737) |   7 |   20 |  3.03% |  1.73% |  900 |  1.91% |  600\n",
      "  2 | 00:34:02 (1006) |   9 |   20 |  4.50% |  1.67% |  1000 |  2.28% |  700\n",
      "  3 | 00:42:35 (513) |   5 |   20 |  2.38% |  1.89% |  400 |  1.93% |  600\n",
      "  4 | 00:47:32 (297) |   3 |  250 |  3.09% |  2.00% |  100 |  1.58% |  800\n",
      "  5 | 00:56:05 (513) |   5 |  250 |  2.22% |  1.88% |  400 |  2.15% |  500\n",
      "  6 | 01:04:37 (512) |   5 |  500 |  2.19% |  1.82% |  400 |  2.08% |  600\n",
      "  7 | 01:21:23 (1006) |   9 |  250 |  4.48% |  1.76% |  1000 |  2.18% |  600\n",
      "  8 | 01:33:41 (738) |   7 |  250 |  3.13% |  1.60% |  1000 |  1.70% |  600\n",
      "  9 | 01:50:25 (1004) |   9 |  500 |  4.48% |  1.71% |  500 |  2.21% |  1000\n",
      " 10 | 01:55:21 (296) |   3 |   20 |  3.09% |  2.00% |  100 |  1.49% |  800\n",
      " 11 | 02:07:34 (733) |   7 |  500 |  3.04% |  1.70% |  900 |  1.61% |   25\n",
      "cvp: [10  8  1  0  7  6  2  3  9 11  5  4]\n",
      "cv_params_: [(9, 250), (7, 500), (3, 250), (3, 20), (7, 250), (7, 20), (3, 500), (5, 20), (9, 20), (9, 500), (5, 500), (5, 250)]\n",
      "\n",
      "Lookahead:  1 | Train: 252 | Test: 63 | Params:  12 | Train configs: 6\n",
      "  0 | 00:03:53 (233) |   9 |  250 |  0.99% |  1.35% |  500 |  1.67% |   50\n",
      "  1 | 00:06:34 (161) |   7 |  500 |  2.77% |  1.52% |  500 |  2.08% |   10\n",
      "  2 | 00:07:57 ( 83) |   3 |  250 |  2.27% |  1.38% |   50 |  2.25% |   75\n",
      "  3 | 00:09:22 ( 85) |   3 |   20 |  2.27% |  1.38% |   50 |  2.25% |   75\n",
      "  4 | 00:12:08 (166) |   7 |  250 |  2.77% |  1.52% |  900 |  2.08% |   10\n",
      "  5 | 00:14:54 (166) |   7 |   20 |  2.77% |  1.50% |  900 |  2.08% |   10\n",
      "  6 | 00:16:18 ( 84) |   3 |  500 |  2.27% |  1.38% |   50 |  2.25% |   75\n",
      "  7 | 00:18:20 (122) |   5 |   20 |  2.88% |  1.51% |  300 |  1.37% |  300\n",
      "  8 | 00:22:24 (244) |   9 |   20 |  0.99% |  1.40% |  500 |  1.67% |   50\n",
      "  9 | 00:26:23 (239) |   9 |  500 |  0.99% |  1.39% |  500 |  1.68% |  400\n",
      " 10 | 00:28:26 (124) |   5 |  500 |  2.88% |  1.48% |  300 |  1.39% |  300\n",
      " 11 | 00:30:29 (122) |   5 |  250 |  2.88% |  1.48% |  300 |  1.39% |  300\n",
      "cvp: [11  3  1  6  9  2  7  5  4 10  0  8]\n",
      "cv_params_: [(9, 500), (5, 20), (3, 250), (7, 20), (9, 20), (3, 500), (7, 250), (5, 500), (5, 250), (9, 250), (3, 20), (7, 500)]\n",
      "\n",
      "Lookahead:  5 | Train: 1134 | Test: 63 | Params:  12 | Train configs: 6\n",
      "  0 | 00:17:10 (1030) |   9 |  500 |  4.64% |  2.32% |   75 |  1.86% |   75\n",
      "  1 | 00:25:41 (510) |   5 |   20 |  4.19% |  3.41% |  300 |  3.12% |  400\n",
      "  2 | 00:30:41 (300) |   3 |  250 |  5.68% |  3.30% |  200 |  3.42% |  200\n",
      "  3 | 00:43:08 (747) |   7 |   20 |  1.75% |  2.06% |  200 |  2.15% |  200\n",
      "  4 | 01:00:14 (1026) |   9 |   20 |  4.64% |  2.32% |   75 |  1.86% |   75\n",
      "  5 | 01:05:15 (301) |   3 |  500 |  5.73% |  3.30% |  200 |  3.42% |  200\n",
      "  6 | 01:17:55 (760) |   7 |  250 |  1.75% |  2.06% |  200 |  2.15% |  200\n",
      "  7 | 01:26:44 (529) |   5 |  500 |  4.19% |  3.41% |  300 |  3.12% |  400\n",
      "  8 | 01:35:38 (534) |   5 |  250 |  3.63% |  3.33% |  300 |  2.84% |  400\n",
      "  9 | 01:52:52 (1034) |   9 |  250 |  4.64% |  2.32% |   75 |  1.86% |   75\n",
      " 10 | 01:57:54 (302) |   3 |   20 |  5.67% |  3.30% |  200 |  3.42% |  200\n",
      " 11 | 02:10:28 (754) |   7 |  500 |  1.75% |  2.06% |  200 |  2.15% |  200\n",
      "cvp: [ 0  2  8  6 10  3  1  9  4 11  5  7]\n",
      "cv_params_: [(3, 20), (3, 500), (7, 500), (7, 20), (9, 250), (5, 20), (3, 250), (9, 20), (5, 250), (9, 500), (5, 500), (7, 250)]\n",
      "\n",
      "Lookahead:  5 | Train: 252 | Test: 63 | Params:  12 | Train configs: 6\n",
      "  0 | 00:01:23 ( 83) |   3 |   20 |  3.53% |  1.18% |   50 |  1.71% |  1000\n",
      "  1 | 00:02:47 ( 84) |   3 |  500 |  3.53% |  1.18% |   50 |  1.71% |  1000\n",
      "  2 | 00:05:36 (169) |   7 |  500 |  3.65% |  1.03% |  900 |  2.17% |  700\n",
      "  3 | 00:08:23 (167) |   7 |   20 |  3.67% |  1.00% |   75 |  2.17% |  700\n",
      "  4 | 00:12:27 (244) |   9 |  250 |  2.90% |  0.94% |   50 |  1.64% |  900\n",
      "  5 | 00:14:29 (122) |   5 |   20 |  3.66% |  1.14% |   50 |  2.21% |  900\n",
      "  6 | 00:15:54 ( 85) |   3 |  250 |  3.53% |  1.18% |   50 |  1.71% |  1000\n",
      "  7 | 00:19:59 (245) |   9 |   20 |  2.91% |  0.95% |  700 |  1.81% |  700\n",
      "  8 | 00:22:02 (123) |   5 |  250 |  3.66% |  1.14% |   50 |  2.21% |  900\n",
      "  9 | 00:26:06 (244) |   9 |  500 |  2.91% |  0.95% |  700 |  1.81% |  700\n",
      " 10 | 00:28:08 (122) |   5 |  500 |  3.66% |  1.14% |   50 |  2.21% |  900\n",
      " 11 | 00:30:57 (168) |   7 |  250 |  3.70% |  1.03% |  1000 |  2.17% |  700\n",
      "cvp: [11  6  9 10  0  5  2  7  3  8  4  1]\n",
      "cv_params_: [(9, 500), (7, 20), (9, 20), (9, 250), (3, 20), (5, 500), (3, 500), (7, 250), (5, 20), (7, 500), (5, 250), (3, 250)]\n",
      "\n",
      "Lookahead: 21 | Train: 1134 | Test: 63 | Params:  12 | Train configs: 6\n",
      "  0 | 00:17:35 (1055) |   9 |  500 |  1.85% |  5.02% |  500 |  5.93% |   75\n",
      "  1 | 00:30:15 (759) |   7 |   20 | -2.02% |  5.58% |   75 |  7.75% |   75\n",
      "  2 | 00:48:08 (1073) |   9 |   20 |  1.85% |  5.02% |  500 |  5.93% |   75\n",
      "  3 | 01:06:08 (1080) |   9 |  250 |  1.85% |  5.02% |  500 |  5.93% |   75\n",
      "  4 | 01:11:02 (294) |   3 |   20 | -0.20% |  8.10% |   75 |  10.13% |   75\n",
      "  5 | 01:19:36 (514) |   5 |  500 |  3.02% |  6.49% |  100 |  8.71% |  100\n",
      "  6 | 01:24:40 (304) |   3 |  500 | -0.20% |  8.10% |   75 |  10.13% |   75\n",
      "  7 | 01:37:23 (763) |   7 |  250 | -2.02% |  5.58% |   75 |  7.75% |   75\n",
      "  8 | 01:46:03 (520) |   5 |   20 |  3.02% |  6.49% |  100 |  8.71% |  100\n",
      "  9 | 01:58:48 (765) |   7 |  500 | -2.02% |  5.58% |   75 |  7.75% |   75\n",
      " 10 | 02:07:29 (520) |   5 |  250 |  3.02% |  6.49% |  100 |  8.71% |  100\n",
      " 11 | 02:12:33 (304) |   3 |  250 | -0.20% |  8.10% |   75 |  10.13% |   75\n",
      "cvp: [ 9  7 11  6  8  0  4 10  1  5  2  3]\n",
      "cv_params_: [(9, 20), (7, 250), (9, 500), (7, 20), (7, 500), (3, 20), (5, 250), (9, 250), (3, 250), (5, 500), (3, 500), (5, 20)]\n",
      "\n",
      "Lookahead: 21 | Train: 252 | Test: 63 | Params:  12 | Train configs: 6\n",
      "  0 | 00:04:15 (255) |   9 |   20 |  1.33% |  2.72% |  1000 |  2.47% |  700\n",
      "  1 | 00:07:14 (179) |   7 |  250 |  1.50% |  2.69% |  1000 |  2.76% |  700\n",
      "  2 | 00:11:30 (256) |   9 |  500 |  1.33% |  2.72% |  1000 |  2.51% |  700\n",
      "  3 | 00:14:28 (178) |   7 |   20 |  1.50% |  2.70% |  1000 |  2.76% |  700\n",
      "  4 | 00:17:26 (178) |   7 |  500 |  1.50% |  2.69% |  1000 |  2.76% |  700\n",
      "  5 | 00:18:58 ( 92) |   3 |   20 |  2.38% |  4.23% |  1000 |  4.93% |  1000\n",
      "  6 | 00:21:10 (132) |   5 |  250 |  2.33% |  3.62% |  300 |  4.21% |  300\n",
      "  7 | 00:25:25 (255) |   9 |  250 |  1.13% |  2.60% |  600 |  2.29% |  900\n",
      "  8 | 00:26:50 ( 85) |   3 |  250 |  2.38% |  4.23% |  1000 |  4.93% |  1000\n",
      "  9 | 00:28:56 (127) |   5 |  500 |  2.33% |  3.62% |  300 |  4.21% |  300\n",
      " 10 | 00:30:21 ( 85) |   3 |  500 |  2.38% |  4.23% |  1000 |  4.93% |  1000\n",
      " 11 | 00:32:26 (125) |   5 |   20 |  2.33% |  3.62% |  300 |  4.21% |  300\n",
      "results:                                      0    y_test        10        25        50  \\\n",
      "600                          0.016727       NaN       NaN       NaN       NaN   \n",
      "700                          0.017464       NaN       NaN       NaN       NaN   \n",
      "800                          0.019177       NaN       NaN       NaN       NaN   \n",
      "900                          0.021039       NaN       NaN       NaN       NaN   \n",
      "1000                         0.023282       NaN       NaN       NaN       NaN   \n",
      "...                               ...       ...       ...       ...       ...   \n",
      "(ZION, 2015-03-27 00:00:00)       NaN  0.055261  0.009229  0.004079  0.004037   \n",
      "(ZION, 2015-03-30 00:00:00)       NaN  0.047778  0.007001  0.000793 -0.000817   \n",
      "(ZION, 2015-03-31 00:00:00)       NaN  0.049444  0.007001  0.001055 -0.000339   \n",
      "(ZION, 2015-04-01 00:00:00)       NaN  0.051615  0.012994  0.012796  0.013031   \n",
      "(ZION, 2015-04-02 00:00:00)       NaN  0.060494  0.012994  0.012919  0.012574   \n",
      "\n",
      "                                   75       100       200       300       400  \\\n",
      "600                               NaN       NaN       NaN       NaN       NaN   \n",
      "700                               NaN       NaN       NaN       NaN       NaN   \n",
      "800                               NaN       NaN       NaN       NaN       NaN   \n",
      "900                               NaN       NaN       NaN       NaN       NaN   \n",
      "1000                              NaN       NaN       NaN       NaN       NaN   \n",
      "...                               ...       ...       ...       ...       ...   \n",
      "(ZION, 2015-03-27 00:00:00)  0.004184  0.004272  0.000538  0.003110  0.005584   \n",
      "(ZION, 2015-03-30 00:00:00)  0.000140 -0.000578 -0.004179 -0.004305 -0.004311   \n",
      "(ZION, 2015-03-31 00:00:00) -0.001378 -0.001378 -0.001175 -0.000292  0.001636   \n",
      "(ZION, 2015-04-01 00:00:00)  0.012243  0.011657  0.010855  0.009279  0.010082   \n",
      "(ZION, 2015-04-02 00:00:00)  0.010871  0.009521  0.008427  0.006566  0.004951   \n",
      "\n",
      "                             ...       600       700       800       900  \\\n",
      "600                          ...       NaN       NaN       NaN       NaN   \n",
      "700                          ...       NaN       NaN       NaN       NaN   \n",
      "800                          ...       NaN       NaN       NaN       NaN   \n",
      "900                          ...       NaN       NaN       NaN       NaN   \n",
      "1000                         ...       NaN       NaN       NaN       NaN   \n",
      "...                          ...       ...       ...       ...       ...   \n",
      "(ZION, 2015-03-27 00:00:00)  ...  0.005993  0.004503  0.004388  0.005769   \n",
      "(ZION, 2015-03-30 00:00:00)  ... -0.010095 -0.013398 -0.014651 -0.015483   \n",
      "(ZION, 2015-03-31 00:00:00)  ... -0.003198 -0.006253 -0.006422 -0.006694   \n",
      "(ZION, 2015-04-01 00:00:00)  ...  0.004553  0.000044 -0.001452 -0.001564   \n",
      "(ZION, 2015-04-02 00:00:00)  ...  0.001086 -0.002953 -0.005753 -0.006171   \n",
      "\n",
      "                                 1000    i  max_depth  min_child_samples  \\\n",
      "600                               NaN  NaN        NaN                NaN   \n",
      "700                               NaN  NaN        NaN                NaN   \n",
      "800                               NaN  NaN        NaN                NaN   \n",
      "900                               NaN  NaN        NaN                NaN   \n",
      "1000                              NaN  NaN        NaN                NaN   \n",
      "...                               ...  ...        ...                ...   \n",
      "(ZION, 2015-03-27 00:00:00)  0.008016  7.0        5.0               20.0   \n",
      "(ZION, 2015-03-30 00:00:00) -0.015232  7.0        5.0               20.0   \n",
      "(ZION, 2015-03-31 00:00:00) -0.006299  7.0        5.0               20.0   \n",
      "(ZION, 2015-04-01 00:00:00) -0.002330  7.0        5.0               20.0   \n",
      "(ZION, 2015-04-02 00:00:00) -0.006676  7.0        5.0               20.0   \n",
      "\n",
      "                             task_type thread_count  \n",
      "600                                NaN          NaN  \n",
      "700                                NaN          NaN  \n",
      "800                                NaN          NaN  \n",
      "900                                NaN          NaN  \n",
      "1000                               NaN          NaN  \n",
      "...                                ...          ...  \n",
      "(ZION, 2015-03-27 00:00:00)        GPU         -1.0  \n",
      "(ZION, 2015-03-30 00:00:00)        GPU         -1.0  \n",
      "(ZION, 2015-03-31 00:00:00)        GPU         -1.0  \n",
      "(ZION, 2015-04-01 00:00:00)        GPU         -1.0  \n",
      "(ZION, 2015-04-02 00:00:00)        GPU         -1.0  \n",
      "\n",
      "[484164 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "for lookahead, train_length, test_length in test_params:\n",
    "    cvp = np.random.choice(list(range(n_params)), size=int(n_params / 1), replace=False)\n",
    "    print(f'cvp: {cvp}')\n",
    "    cv_params_ = [cv_params[i] for i in cvp]\n",
    "    print(f'cv_params_: {cv_params_}\\n')\n",
    "\n",
    "    n_splits = int(2 * YEAR / test_length)\n",
    "    print(f'Lookahead: {lookahead:2.0f} | Train: {train_length:3.0f} | '\n",
    "          f'Test: {test_length:2.0f} | Params: {len(cv_params_):3.0f} | Train configs: {len(test_params)}')\n",
    "\n",
    "    cv = MultipleTimeSeriesCV(n_splits=n_splits, lookahead=lookahead, test_period_length=test_length,\n",
    "                              train_period_length=train_length)\n",
    "\n",
    "    label = label_dict[lookahead]\n",
    "    outcome_data = data.loc[:, features + [label]].dropna()\n",
    "    cat_cols_idx = [outcome_data.columns.get_loc(c) for c in categoricals]\n",
    "    catboost_data = Pool(label=outcome_data[label], data=outcome_data.drop(label, axis=1),\n",
    "                         cat_features=cat_cols_idx)\n",
    "    predictions, metrics, feature_importance, daily_ic = [], [], [], []\n",
    "    key = f'{lookahead}/{train_length}/{test_length}'\n",
    "    T = 0\n",
    "    for p, param_vals in enumerate(cv_params_):\n",
    "        params = dict(zip(param_names, param_vals))\n",
    "        params['task_type'] = 'GPU'\n",
    "        params['thread_count'] = -1\n",
    "\n",
    "        start = time()\n",
    "        cv_preds, nrounds = [], []\n",
    "        ic_cv = defaultdict(list)\n",
    "        for i, (train_idx, test_idx) in enumerate(cv.split(X=outcome_data)):\n",
    "            train_set = catboost_data.slice(train_idx.tolist())\n",
    "\n",
    "            model = CatBoostRegressor(**params)\n",
    "            model.fit(X=train_set, verbose_eval=False)\n",
    "\n",
    "            test_set = outcome_data.iloc[test_idx, :]\n",
    "            X_test = test_set.loc[:, model.feature_names_]\n",
    "            y_test = test_set.loc[:, label]\n",
    "            y_pred = {str(n): model.predict(X_test, ntree_end=n) for n in num_iterations}\n",
    "            cv_preds.append(y_test.to_frame('y_test').assign(**y_pred).assign(i=i))\n",
    "\n",
    "        cv_preds = pd.concat(cv_preds).assign(**params)\n",
    "\n",
    "        predictions.append(cv_preds)\n",
    "        by_day = cv_preds.groupby(level='date')\n",
    "        ic_by_day = pd.concat([by_day.apply(lambda x: spearmanr(x.y_test, x[str(n)])[0]).to_frame(n)\n",
    "                               for n in num_iterations], axis=1)\n",
    "        daily_ic_mean = ic_by_day.mean()\n",
    "        daily_ic_mean_n = daily_ic_mean.idxmax()\n",
    "        daily_ic_median = ic_by_day.median()\n",
    "        daily_ic_median_n = daily_ic_median.idxmax()\n",
    "\n",
    "        ic = [spearmanr(cv_preds.y_test, cv_preds[str(n)])[0] for n in num_iterations]\n",
    "        t = time() - start\n",
    "        T += t\n",
    "        metrics = pd.Series(list(param_vals) + [t, daily_ic_mean.max(), daily_ic_mean_n,\n",
    "                             daily_ic_median.max(), daily_ic_median_n] + ic, index=metric_cols)\n",
    "        msg = f'{p:3.0f} | {format_time(T)} ({t:3.0f}) | {params[\"max_depth\"]:3.0f} | '\n",
    "        msg += f'{params[\"min_child_samples\"]:4.0f} | {max(ic):6.2%} | {ic_by_day.mean().max(): 6.2%} | '\n",
    "        msg += f'{daily_ic_mean_n: 4.0f} | {ic_by_day.median().max(): 6.2%} | {daily_ic_median_n: 4.0f}'\n",
    "        print(msg)\n",
    "        metrics.to_hdf(cb_store, 'metrics/' + key)\n",
    "        ic_by_day.assign(**params).to_hdf(cb_store, 'daily_ic/' + key)\n",
    "        cv_preds.to_hdf(cb_store, 'predictions/' + key)\n",
    "\n",
    "\n",
    "print(\"results: \" , pd.concat([metrics.tail(), cv_preds]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "                     y_test        10        25        50        75       100  \\\nsymbol date                                                                     \nZION   2015-03-27  0.055261  0.009229  0.004079  0.004037  0.004184  0.004272   \n       2015-03-30  0.047778  0.007001  0.000793 -0.000817  0.000140 -0.000578   \n       2015-03-31  0.049444  0.007001  0.001055 -0.000339 -0.001378 -0.001378   \n       2015-04-01  0.051615  0.012994  0.012796  0.013031  0.012243  0.011657   \n       2015-04-02  0.060494  0.012994  0.012919  0.012574  0.010871  0.009521   \n\n                        200       300       400       500       600       700  \\\nsymbol date                                                                     \nZION   2015-03-27  0.000538  0.003110  0.005584  0.004903  0.005993  0.004503   \n       2015-03-30 -0.004179 -0.004305 -0.004311 -0.009913 -0.010095 -0.013398   \n       2015-03-31 -0.001175 -0.000292  0.001636 -0.003703 -0.003198 -0.006253   \n       2015-04-01  0.010855  0.009279  0.010082  0.004626  0.004553  0.000044   \n       2015-04-02  0.008427  0.006566  0.004951 -0.000361  0.001086 -0.002953   \n\n                        800       900      1000  i  max_depth  \\\nsymbol date                                                     \nZION   2015-03-27  0.004388  0.005769  0.008016  7          5   \n       2015-03-30 -0.014651 -0.015483 -0.015232  7          5   \n       2015-03-31 -0.006422 -0.006694 -0.006299  7          5   \n       2015-04-01 -0.001452 -0.001564 -0.002330  7          5   \n       2015-04-02 -0.005753 -0.006171 -0.006676  7          5   \n\n                   min_child_samples task_type  thread_count  \nsymbol date                                                   \nZION   2015-03-27                 20       GPU            -1  \n       2015-03-30                 20       GPU            -1  \n       2015-03-31                 20       GPU            -1  \n       2015-04-01                 20       GPU            -1  \n       2015-04-02                 20       GPU            -1  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>y_test</th>\n      <th>10</th>\n      <th>25</th>\n      <th>50</th>\n      <th>75</th>\n      <th>100</th>\n      <th>200</th>\n      <th>300</th>\n      <th>400</th>\n      <th>500</th>\n      <th>600</th>\n      <th>700</th>\n      <th>800</th>\n      <th>900</th>\n      <th>1000</th>\n      <th>i</th>\n      <th>max_depth</th>\n      <th>min_child_samples</th>\n      <th>task_type</th>\n      <th>thread_count</th>\n    </tr>\n    <tr>\n      <th>symbol</th>\n      <th>date</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">ZION</th>\n      <th>2015-03-27</th>\n      <td>0.055261</td>\n      <td>0.009229</td>\n      <td>0.004079</td>\n      <td>0.004037</td>\n      <td>0.004184</td>\n      <td>0.004272</td>\n      <td>0.000538</td>\n      <td>0.003110</td>\n      <td>0.005584</td>\n      <td>0.004903</td>\n      <td>0.005993</td>\n      <td>0.004503</td>\n      <td>0.004388</td>\n      <td>0.005769</td>\n      <td>0.008016</td>\n      <td>7</td>\n      <td>5</td>\n      <td>20</td>\n      <td>GPU</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>2015-03-30</th>\n      <td>0.047778</td>\n      <td>0.007001</td>\n      <td>0.000793</td>\n      <td>-0.000817</td>\n      <td>0.000140</td>\n      <td>-0.000578</td>\n      <td>-0.004179</td>\n      <td>-0.004305</td>\n      <td>-0.004311</td>\n      <td>-0.009913</td>\n      <td>-0.010095</td>\n      <td>-0.013398</td>\n      <td>-0.014651</td>\n      <td>-0.015483</td>\n      <td>-0.015232</td>\n      <td>7</td>\n      <td>5</td>\n      <td>20</td>\n      <td>GPU</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>2015-03-31</th>\n      <td>0.049444</td>\n      <td>0.007001</td>\n      <td>0.001055</td>\n      <td>-0.000339</td>\n      <td>-0.001378</td>\n      <td>-0.001378</td>\n      <td>-0.001175</td>\n      <td>-0.000292</td>\n      <td>0.001636</td>\n      <td>-0.003703</td>\n      <td>-0.003198</td>\n      <td>-0.006253</td>\n      <td>-0.006422</td>\n      <td>-0.006694</td>\n      <td>-0.006299</td>\n      <td>7</td>\n      <td>5</td>\n      <td>20</td>\n      <td>GPU</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>2015-04-01</th>\n      <td>0.051615</td>\n      <td>0.012994</td>\n      <td>0.012796</td>\n      <td>0.013031</td>\n      <td>0.012243</td>\n      <td>0.011657</td>\n      <td>0.010855</td>\n      <td>0.009279</td>\n      <td>0.010082</td>\n      <td>0.004626</td>\n      <td>0.004553</td>\n      <td>0.000044</td>\n      <td>-0.001452</td>\n      <td>-0.001564</td>\n      <td>-0.002330</td>\n      <td>7</td>\n      <td>5</td>\n      <td>20</td>\n      <td>GPU</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>2015-04-02</th>\n      <td>0.060494</td>\n      <td>0.012994</td>\n      <td>0.012919</td>\n      <td>0.012574</td>\n      <td>0.010871</td>\n      <td>0.009521</td>\n      <td>0.008427</td>\n      <td>0.006566</td>\n      <td>0.004951</td>\n      <td>-0.000361</td>\n      <td>0.001086</td>\n      <td>-0.002953</td>\n      <td>-0.005753</td>\n      <td>-0.006171</td>\n      <td>-0.006676</td>\n      <td>7</td>\n      <td>5</td>\n      <td>20</td>\n      <td>GPU</td>\n      <td>-1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_preds.tail()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}